[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de datos",
    "section": "",
    "text": "Prefacio\nEl presente libro es un compilado de lecturas y actividades sobre diferentes temas que son considerados fundamentales para el análisis de datos. Cada capítulo corresponde a un tema e incluye un resumen con ideas y conclusiones a destacar.\nEste material fue desarrollado exclusivamente para las materias Análisis de Datos I y II de la Diplomatura Universitaria en Programación de la UTN HAEDO, dictada inicialmente en 2023."
  },
  {
    "objectID": "01 - conceptos y definiciones.html",
    "href": "01 - conceptos y definiciones.html",
    "title": "Conceptos y definiciones",
    "section": "",
    "text": "Lectura\n\nhttps://aws.amazon.com/es/what-is/data-analytics/\nhttps://www.intel.la/content/www/xl/es/artificial-intelligence/what-is-data-analytics.html\nhttps://www.oracle.com/business-analytics/data-analytics/\n\n\n\nActividades\n\n¿Qué es Data Analytics? ¿Considera que es lo mismo que Análisis de Datos?\n¿En qué se diferencia de Big Data Analytics?\n¿Cuáles son los pasos a seguir en un proceso de análisis de datos?\nMencione al menos 3 herramientas vinculadas al análisis de datos\n¿Qué tipo de análisis de datos se pueden hacer?\nRelacione la anterior respuesta con los siguientes conceptos: raw data, information, knowledge, insight, actionable insight, decision-making, model\nDiferencie brevemente Machine Learning, Data Mining, Data Science, Business Intelligence, Data Analysis y Data Analytics\n\n\n\nResumen\nEl análisis de datos es una actividad que se encuentra integrada a otras como ingeniería y ciencia de datos compartiendo conceptos y herramientas, y habitualmente superponiéndose en el día a día de muchas organizaciones. Por esto mismo, como profesional del área, es importante reconocer cuáles son las incumbencias del análisis de datos y cómo se relaciona con el resto del ciclo de vida de los datos.\nSurge entonces la necesidad de definir y comprender conceptos fundamentales para lograr esa distinción. Como suele suceder en otros rubros, varios de estos conceptos son asimilados del inglés y se utilizan diariamente de esta manera, lo que contribuye a la confunsión en algunos casos. Además, las definiciones no siempre son unísonas: dependen de cada autor y evolucionan en el tiempo. Implica así una capacitación y actualización constante.\nComo actividad, el análisis de datos requiere principalmente conocimientos de estadística y de dominio (o de negocio), de tal manera de poder trabajar los datos, obtener conclusiones y traducir esas conclusiones en recomendaciones o acciones (o simplemente producir información / conocimiento para su posterior tratamiento).\nEl rol no necesariamente implica el uso de lenguajes de programación, aunque (en el estadío actual del área) suele ser altamente recomendable adquirir su uso. De no contar con ello, es esencial la experiencia en al menos una herramienta de visualización (Tableau, PowerBI, Looker) y uso avanzado de planillas de cálculo (Excel, Google Sheets).\nRetomando lo dicho, es habitual también la necesidad de un cierto de grado de transformación de los datos. En ese aspecto, Python, R y SQL son herramientas de gran ayuda, que desde ya pueden utilizarse más adelante en el análisis como tal. De hecho, su uso se ha consolidado cada vez más de forma mandatoria en términos de poder trabajar con grandes volúmenes de datos y elaborar modelos complejos, inviables de implementar con otras herramientas como las mencionadas anteriormente.\nPosteriormente al análisis, la presentación de los mismos es sumamente importante, y es incluso un diferencial. En ese sentido, un analista de datos completo cuenta con la capacidad para la elaboración de informes y visualizaciones, pudiendo abstraerse de la parte técnica y mostrar efectivamente y de manera sencilla los resultados de su análisis.\nRecapitulando, el análisis de datos pasa a ser hoy un conjunto de conocimientos y técnicas que, con las herramientas adecuadas y enmarcado correctamente en un proceso de analítica de datos, permite la toma de decisiones dentro de las organizaciones a través de la exploración e inferencia de los datos."
  },
  {
    "objectID": "02 - datos unidades y variables.html",
    "href": "02 - datos unidades y variables.html",
    "title": "Datos, unidades y variables de análisis",
    "section": "",
    "text": "Lectura\n\nLos Datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 13 a 15)\nLas Variables (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 15 a 17)\nLa Primera Organización de los Datos: la matriz de datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 25 a 27)\nTipos de variables (Estadística Aplicada a los Negocios y la Economía: pág. 8 y 9)\nVariables aleatorias (Estadística Aplicada a los Negocios y la Economía: pág. 189 y 190)\nTypes of Data (STAT 414 | Introduction to Probability Theory: lesson 1.4)\nEl concepto de variable aleatoria (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 52 y 53)\n\n\n\nActividades\n\nBusque y analice algún artículo que hable acerca de la diferencia entre datos, información y conocimiento. Provea un ejemplo propio que muestre dicha diferencia.\nConsidere el dataset Top Trends on TikTok & YoutubeShorts. Determine la unidad de análisis, las variables y su tipo. ¿Es alguna de ellas aleatoria?\n\n\n\nResumen\nLos datos son el insumo básico para el análisis de datos. La mínima unidad de información, la materia prima a partir de la cual se produce conocimiento y se pueden tomar acciones basadas en evidencia con cierto grado medible de incertidumbre.\nEs adecuado pensar a un dato como aquello que puede colocarse dentro de una celda (de una planilla); esto es, un número, una letra, una palabra, un verdadero o falso. El hecho de considerar a algo como un dato dependerá del análisis mismo, porque ese dato surgirá como resultado del entrecruzamiento de 2 conceptos sumamente importantes: la unidad de análisis y la variable.\nLa unidad de análisis es el objeto o sujeto sobre el cual se lleva adelante el análisis. La variable es la característica de interés de ese objeto o sujeto, sobre el cual se observará o medirá un dato. La primera suele identificarse como cada una de las filas (de una planilla), la segunda como las columnas (ver imagen); aunque no siempre tiene por qué ser así. La lógica entonces de cualquier análisis de datos será identificar la unidad de análisis, la o las variables de estudio y obtener de estas variables uno o más datos.\n\n\n\n\nvariable 1\nvariable 2\nvariable 3\n\n\n\n\nunidad 1\ndato\ndato\ndato\n\n\nunidad 2\ndato\ndato\ndato\n\n\nunidad 3\ndato\ndato\ndato\n\n\n\nLas variables pueden ser de 2 tipos: cuantitativas o cualitativas. Las primeras, asimismo, pueden ser continuas o discretas, y las segundas, ordinales o nominales. Lo importante de reconocer qué tipo de variable se tiene a la mano está en el hecho de anticipar las técnicas y limitaciones o no para cada caso. Las variables más flexibles y potentes son las cuantitativas continuas porque las mismas pueden discretizarse, ordenarse o transformarse en cualitativas, no así al revés.\nPor otra parte, la clasificación de variables puede implicar no sólo su tipo, sino también si es resultado o no de un proceso aleatorio. Es decir, si es producto del azar en algún punto de su obtención. Esto traerá apareado un cambio completo en el modo de analizar a la variable, así como también de las conclusiones que pueden extraerse.\nRecapitulando, como analista de datos, es sumamente importante participar o tener conocimiento pleno en la definición exhaustiva de la unidad de análisis, del recocimiento de las variables y de la obtención de datos, ya que estos 3 puntos son la base sobre la cual se construye por completo cualquier análisis."
  },
  {
    "objectID": "03 - tablas de frecuencias.html",
    "href": "03 - tablas de frecuencias.html",
    "title": "Tablas de frecuencias",
    "section": "",
    "text": "Lectura\n\nConstrucción de una tabla de frecuencias (Estadística Aplicada a los Negocios y la Economía: pág. 23 y 24)\nConstrucción de distribuciones de frecuencias: datos cuantitativos (Estadística Aplicada a los Negocios y la Economía: pág. 29 a 34)\nProblema de Trabajo e Investigación Estadística (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 12 y 13)\nEl Análisis de la Matriz de Datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 28)\nLas Distribuciones de Frecuencias en el Análisis Univariado (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 29 a 45)\n\n\n\nActividades\n\nConsidere el set de datos Top Trends on TikTok & YoutubeShorts. Elabore tablas de frecuencias absolutas, relativas y acumuladas (si corresponde) para todas las variables.\nReleve el uso e implementación de tablas pivote en diferentes softwares/plataformas (ej: Excel, Google Sheets, Python, R, entre otros). Utilice alguno de los ejemplos sugeridos para realizar las anteriores tablas.\nElabore al menos 3 preguntas de negocio que podrían ser respondidas con las anteriores tablas.\n\n\n\nResumen\nLa forma más básica de analizar / resumir un conjunto de datos es una tabla de frecuencias. Implica la operación de conteo de unidades de análisis o de sus variables, agrupadas por algún criterio, conformando categorías o clases excluyentes (sin solapamiento). En las variables cualtitativas, suele ser bastante directo, siendo que las categorías se encuentran naturalmente identificadas; en las cuantitativas dependerá del análisis, pudiendo agrupar por intervalos del mismo o diferente tamaño, según se necesite.\nEl resultado puntual del conteo para cada clase o intervalo se denomina frecuencia y el conjunto de éstas se conoce como distribución de frecuencias. La frecuencia puede ser absoluta o relativa (si es que se divide por el total de unidades o total de la variable), dando una mejor idea de cuán relevante es cierta clase respecto del total y además permitiendo comparar con otras variables. La suma de la columna con las frecuencias absolutas debe dar siempre como resultado el conteo de unidades total, mientras que la relativa debe dar siempre 1.\nAsí, la tabla de frecuencias puede construirse inicialmente de la siguiente manera: una columna con las clases / categorías / intervalos, una columna con las frecuencias absolutas y otra columna con las relativas. Éstas 2 últimas suelen acompañarse de una fila con el total, de tal manera de verificar lo anteriormente dicho. Por sobre esta configuración básica, se puede agregar una columna con el punto medio de clase (habitualmente la diferencia entre valor máximo y mínimo de un intervalo, dividido 2) o punto máximo, lo cual ayuda a identifar rápidamente un intervalo sin recurrir a su definición por intervalo (ambos máximo y mínimo), y también siendo soporte para una posterior visualización. Adicionalmente, también puede utilizarse otra columna con el ancho de clase ó amplitud (diferencia entre valor máximo y mínimo).\nA continuación un ejemplo, realizado en Google Sheets, de la construcción de tablas de frecuencias con variable cualitativa y cuantitativa. Se recomienda explorar las funciones utilizadas en cada caso.\n\n\nPara responder algunas preguntas del análisis puede llegar a ser necesario el cálculo además de frecuencias acumuladas (esto únicamente para variables susceptibles de ordenamiento). Esencialmente, una vez ordenada la tabla por algún criterio, se procede a la suma fila por fila de las frecuencias absolutas y/o relativas. Sobre estas nuevas columnas no tiene sentido realizar una suma, pero sí se debe constatar que el resultado de la última fila sumada dé el total de la frecuencia absoluta o relativa, según se haya utilizada una u otra.\nPudiendo realizarse de forma manual, en conjuntos grandes de datos, es posible recurrir al uso de tablas pivote o alguna implementación específica para tablas de frecuencia (ej: función FREQUENCY en Google Sheets).\nFinalizando lo referente a la construcción de la tabla de frecuencias, lo más importante a destacar es su flexibilidad y alcance frente a cualquier conjunto de datos. Se recomienda su implementación en cada análisis, para todas las variables, permitiendo posiblemente detectar problemáticas en las mismas o incluso sacar algunas primeras conclusiones. Además, serán necesarias para la realización de algunos tipos de análisis específicos vistos más adelante."
  },
  {
    "objectID": "04 - descripcion de variables.html",
    "href": "04 - descripcion de variables.html",
    "title": "Descripción de variables",
    "section": "",
    "text": "Lectura\n\nMedidas de posición: promedio, mediana, cuantiles y moda\n\nMedidas Resumen (Estadística para Todos: pág. 120 a 125)\nLos valores que caracterizan al conjunto de datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 59 a 79)\nDescripción de datos (Estadística Aplicada a los Negocios y la Economía: pág. 57 a 75)\nMedidas numéricas descriptivas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 11 a 14)\n\n\n\nMedidas de dispersión: desviación estándar, coeficiente de variación y rango\n\nMedidas Resumen (Estadística para Todos: pág. 125 a 135)\nAnálisis de la variación y asimetría (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 85 a 97)\nDescripción de datos (Estadística Aplicada a los Negocios y la Economía: pág. 75 a 84)\nMedidas numéricas descriptivas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 15 a 22)\n\n\n\nMedidas de relación: razones, tasas y proporciones\n\nRazón, tasas y porcentajes (Estadística para Todos: pág. 15 a 19)\n\n\n\n\nActividades\n\nConsidere el dataset Best Movie by Year Netflix. Calcule medidas de posición y dispersión para la variable score. Divida el set de datos a partir de algún criterio que considere de interés, calcule medidas de relación entre los grupos resultantes.\n\n\n\nResumen\nEn el proceso de caracterización / resumen de variables, el análisis exploratorio de datos, participan medidas de 3 cuestiones fundamentales: posición, dispersión y relación.\nLa posición esencialmente permite conocer dónde se encuentran la mayor parte de los datos y la dispersión, saber si los datos que conforman esa mayoría se encuentran cerca unos de otros. Cuando la dispersión aumenta, las medidas de posición realmente no dicen mucho (aunque parezca que sí); por esto mismo, siempre deben estar ambas presentes. Justamente, como la dispersión impacta sobre la posición, se cuenta con una batería de posibles medidas que son más robustas frente a una situación de gran dispersión ó dispersión particular de algunos datos. La más utilizada es la mediana, que a diferencia del promedio (media) logra mantenerse firme frente a valores extremos.\nLa dispersión así también puede ser engañosa, porque depende de las unidades en la que está medida. Por esto mismo, es recomendable el uso del coeficiente de variación, que es una relativización del desvío estándar frente al valor del promedio. Como su nombre lo indica, es un coeficiente y no tiene unidades. Un valor menor a 0.3 (30%) indica baja dispersión y, por ende, las medidas de posición serán confiables.\nAún así, se verá en el próximo capítulo, que aún dándose esta última situación, contar sólo con las medidas de posición no es 100% seguro. Siempre debe recurrise a la visualización para lograr entender el comportamiento de una variable o conjunto de datos, acompañada de una tabla de frecuencias.\nMás allá del análisis de una variable por si sola, también puede presentarse la situación en la que se quiere comparar con otra o incluso comparar consigo misma en otra circunstancia (o en otro momento). Para ello, existen medidas de relación, que son habituales en todo tipo de informes. No se menciona en este capítulo el coeficiente de correlación, que merece un desarrollo más profundo de su uso e implicancias, lo cual se verá más adelante."
  },
  {
    "objectID": "05 - visualizacion.html",
    "href": "05 - visualizacion.html",
    "title": "Visualización de datos",
    "section": "",
    "text": "Lectura\n\nVizualización de cantidades, proporciones y distribuciones\n\nFundamentals of Data Visualization (Part I: Cap. 2, 5, 6, 7 y 10) (complementar con el resto de la bibliografía habitual de ser necesario)\n\n\n\nConceptos básicos de storytelling y presentaciones efectivas\n\nFundamentals of Data Visualization (Part III: Cap. 28 y 29)\n\n\n\n\nActividades\n\nConsidere el dataset French Bakery Daily Sales. Realice visualizaciones que puedan responder a las siguientes preguntas:\n\n¿Cuál fue el total de baguettes tradicionales vendidas en el período Sep-2022?\n¿Cuánto representa la anterior cantidad respecto a todo tipo de baguettes?\n¿Ha habido cambios en esas proporciones respecto a Sep-2021?\n¿Cuál fue el día de semana y horario de mayor ganancia?\n¿Cuánto representa la venta de baguettes tradicionales en dicha ganancia?\n¿Cuál es la variación respecto a unidades vendidas para todo tipo de baguette a lo largo del año 2021?\n¿Cuál es el mayor incremento intermensual de baguettes tradicionales vendidas?\n¿En qué precio se encuentran la mayoría de los productos? ¿y el 80% de los productos?\n\n(ENTREGA OBLIGATORIA) Considere el dataset French Bakery Daily Sales. Asuma que usted es un analista de datos recientemente contratada/o por la panadería francesa. El dueño busca entender el potencial de venta de la baguette tradicional. Realice una presentación de negocio con las visualizaciones anteriores que considere relevantes (modifíque ó cree otras si considera necesario) con el objetivo de describir la situación histórica y actual de la venta de dicho producto. No utilice más de 5 gráficos/diapositivas, debe ser breve y comunicar sólo cuestiones claves. Extraiga una conclusión/recomendación para el dueño.\n\n\n\nResumen\nEn el área de análisis de datos, puede llegar a ser un rol aparte el de aquel que realiza visualizaciones. Es un área que implica no sólo conocimiento técnico en datos, sino en diseño. Sino se tiene esta formación puntual, muchas veces se adquiere con la experiencia, y aún así, frente a una presentación, el contenido y la forma dependerá de quién reciba la información. Lo recomendable es inicialmente partir de un diseño mínimo, replicable y tomar referencias de otras presentaciones. Complementar y mejorar luego en función de la asimilación de principios básicos del diseño, colores e incluso tendencias.\nEn cuanto a los datos, las visualizaciones estarán dadas por la cantidad de variables y su tipo. Suele ser buen hábito recurrir a una galería de visualizaciones para corrobar cuál gráfico encaja con lo que se tiene disponible o se quiere mostrar (ej: Python Graph Gallery). Siempre adecuar la visualización a la pregunta que debe responder y evitar la sobrecarga / repetición de datos. Acompañar con el correspondiente título, referencias y leyenda, así como eventualmente, si fuese necesario, con una breve y sencilla explicación coloquial.\nDicho esto, es posible realizar visualizacion tanto con herramientas pensadas para tal fin (Tableau, PowerBI, Looker), otras más genéricas de presentaciòn (PowerPoint, Google Slides) o con Python / R / Javascript (a través de las librerías adecuadas). En ese sentido, lo recomendable es adoptar una en particular y profundizar en su uso."
  },
  {
    "objectID": "06 - origen y recoleccion.html",
    "href": "06 - origen y recoleccion.html",
    "title": "Origen y recolección de datos",
    "section": "",
    "text": "Lectura\n\nOrigen de los datos (Estadística para Todos: pág. 51 y 52)\nMuestreo (Estadística para Todos: pág. 29 a 35)\nFuentes de datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 18 a 20)\nMétodos de muestreo (Estadística Aplicada a los Negocios y la Economía: pág. 266 a 268)\nMuestras aleatorias (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 214 a 217)\nMétodos de recolección de datos para una investigación\n\n\n\nActividades\n\nConsidere el dataset Pokemon. Lleve adelante la descripción de 2 variables numéricas (cálculo de media y desvío estándar) y 2 categóricas (cálculo de alguna proporción de interés). Luego extraiga una serie de muestras bajo algún criterio que le resulte adecuado. Vuelva a calcular los valores anteriores, compare y extraiga conclusiones.\n\n\n\nResumen\nEl origen y recolección de los datos determina las conclusiones que pueden extraerse de los mismos. Tener un censo (un relevamiento completo de las unidades de análisis) no será lo mismo que tener una muestra (un relevamiento parcial); la segunda lleva consigo la incertidumbre propia de no contar con todos los datos. Pero justamente será ese el escenario donde el análisis de datos estará más presente.\nUna muestra será habitualmente la opción más viable para conseguir datos. Incluso en la era del big data, por más datos que se tengan, suelen ser representaciones parciales de la realidad, de los procesos que los generan. La muestra es la regla, no la excepción.\nEs un procedimiento mediante el cual se extrae, de forma aleatoria, una serie de unidades de análisis de un conjunto mayor (finito o infinito) de unidades. En su versión más simple, consiste en extraer directamente las unidades, aunque ésta puede no ser la mejor forma de hacerlo. Más adelante se verán otras metodologías que, en esencia, buscan mejorar la representatividad de los datos obtenidos respecto del conjunto total, o lo que es igual, disminuir los errores debidos al muestreo mismo. La aleatoriedad juega aquí un rol fundamental, siendo el canal por el cual se busca eliminar todo otro tipo de errores debido a la subjetividad o cuestiones operacionales.\nUn muestreo dará como resultado variables necesariamente aleatorias; el objetivo será caracterizarlas, pero teniendo en cuenta su naturaleza azarosa. Porque cada vez que se tome una muestra, los resultados de un promedio o una desviación estándar (u otra medida) para determinada variable, serán distintos. Todas y cada una de las veces, siempre será distinto. Estos resultados en sí, pueden asimismo considerarse nuevas variables y también caracterizarse. Justamente este es uno de los grandes pilares del análisis de datos.\nRecapitulando, lo que será de interés de aquí en adelante, es la capacidad de lidiar con una muestra y obtener conclusiones de las variables a partir de la misma. En el siguiente capítulo, se hará una introducción a la teoría y conceptos de probabilidad, que serán de gran ayuda."
  },
  {
    "objectID": "07 - probabilidad aplicada.html",
    "href": "07 - probabilidad aplicada.html",
    "title": "Probabilidad aplicada",
    "section": "",
    "text": "Lectura\n\nEpílogo: Estadística y Probabilidad (Estadística para Todos: pág. 245 a 247)\nEstudio de los conceptos de la probabilidad (Estadística Aplicada a los Negocios y la Economía: pág. 144 a 151)\nDistribuciones de Probabilidad Discreta (Estadística Aplicada a los Negocios y la Economía: pág. 186 a 202)\nDistribuciones de Probabilidad Continua (Estadística Aplicada a los Negocios y la Economía: pág. 222 a 242)\nProbability (STAT 500 | Applied Statistics: lesson 2)\nProbability Distributions (STAT 500 | Applied Statistics: lesson 3)\nSampling Distributions (STAT 500 | Applied Statistics: lesson 4)\n\n\n\nActividades\n\nConsidere el dataset Pokemon. Proponga 3 experimentos sobre dicha población para obtener como resultado una variable aleatoria cuya distribución sea: (1) binomial, (2) normal y (3) chi cuadrado.\nResponda las siguientes preguntas:\n\n¿Cuál es la probabilidad de obtener un pokemon cuya velocidad sea mayor a 100?\n¿Cuál es la probabilidad de obtener un pokemon cuyo ataque sea igual a 60?\n¿Cuál es la probabilidad de obtener un pokemon de aire al realizar una muestra aleatoria de 10 pokemones?\n¿Cuál es la probabilidad de obtener un pokemon de generación 3 al seleccionar un sólo pokemon?\nDado que un pokemon es de tierra, ¿cuál es la probabilidad que su defensa sea menor a 50?\n¿Cuál es la probabilidad de obtener un pokemon de tierra y con defensa menor a 50?\n¿Cuál es la probabilidad de que un pokemon sea de tierra suponiendo que en una muestra de 10 unidades obtuvo sólo 1 pokemon de este tipo?\n\n\n\n\nResumen\nLa probabilidad es una herramienta fundamental para el análisis de datos, no tanto por su uso directo, sino porque es el sustento matemático de muchas demostraciones de procesos y fórmulas que habitualmente se aplican. Por tanto, conocer los conceptos y definiciones básicos es una puerta a comprender mejor tanto el análisis como las conclusiones obtenidas.\nSerá particularmente útil asimilar las distribuciones más relevantes que surgen como resultado del análisis de variables aleatorias; éstas son: la distribución binomial, la distribución normal y la distribución chi-cuadrado. Cada una estará asociada al estudio de medidas de posición, dispersión y relación, vistas anteriormente. Es decir, estas medidas, si se dan las adecuadas condiciones, tendrán un comportamiento teóricamente definido y demostrado, lo cual posibilita enormemente su análisis. También será útil el conocimiento y cálculo de probabilidades conjuntas y condicionales, habituales en el análisis multivariado.\nA continuación un ejemplo, realizado en python, de cómo se construye la distribución de una media muestral. Prestar atención particularmente a la forma final de la curva. Tener en cuenta además que la misma dependerá de factores como la cantidad de muestras y el tamaño de éstas.\nInicialmente se ingestan los datos y se calcula la media para la variable speed.\n\nimport pandas as pd\n\ndata = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRMQoYDMXx2r2fIUhy0JbiDI6UyWwTPLEx_mAeXgxEnFikqYu-k0l2BgCv8y_QPsrpWBNSdn8SZBeuT/pub?gid=969789387&single=true&output=csv')\ndisplay(data[['name', 'speed']])\n\nprint('La media de la velocidad es: ', data['speed'].mean())\n\n\n\n\n\n  \n    \n      \n      name\n      speed\n    \n  \n  \n    \n      0\n      Bulbasaur\n      45\n    \n    \n      1\n      Ivysaur\n      60\n    \n    \n      2\n      Venusaur\n      80\n    \n    \n      3\n      VenusaurMega Venusaur\n      80\n    \n    \n      4\n      Charmander\n      65\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      795\n      Diancie\n      50\n    \n    \n      796\n      DiancieMega Diancie\n      110\n    \n    \n      797\n      HoopaHoopa Confined\n      70\n    \n    \n      798\n      HoopaHoopa Unbound\n      80\n    \n    \n      799\n      Volcanion\n      70\n    \n  \n\n800 rows × 2 columns\n\n\n\nLa media de la velocidad es:  68.2775\n\n\nAl tomar una muestra de 100 unidades, el resultado es bastante cercano al valor obtenido previamente. Éste valor, como se ha mencionado anteriormente, variará de muestra en muestra.\n\nprint('La media MUESTRAL de la velocidad es: ', data['speed'].sample(100).mean())\n\nLa media MUESTRAL de la velocidad es:  63.28\n\n\nAhora bien, al realizar varias muestras sucesivas, por ejemplo 100, obtendremos un conjunto de valores de medias muestrales que podemos representar gráficamente.\n\nmedias_muestrales = []\n\nfor muestra in range(100):\n  media_muestral = data['speed'].sample(100).mean()\n  medias_muestrales.append(media_muestral)\n\npd.DataFrame(medias_muestrales, columns=['Distribución de la media muestral para la velocidad']).hist(bins=30, grid=False);\n\n\n\n\nSe observan que la mayoría de los valores se hayan en los alrededores de la media calculada inicialmente. Esta situación se presenta incluso si faltase una única unidad en el muestreo.\n\nmedias_muestrales = []\n\nfor muestra in range(100):\n  media_muestral = data['speed'].sample(799).mean()\n  medias_muestrales.append(media_muestral)\n\npd.DataFrame(medias_muestrales, columns=['Distribución de la media muestral para la velocidad']).hist(bins=30, grid=False);\n\n\n\n\nAl incrementar la cantidad de repeticiones, sucede lo siguiente:\n\nmedias_muestrales = []\n\nfor muestra in range(10000):\n  media_muestral = data['speed'].sample(100).mean()\n  medias_muestrales.append(media_muestral)\n\npd.DataFrame(medias_muestrales, columns=['Distribución de la media muestral para la velocidad']).hist(bins=30, grid=False);\n\n\n\n\nVisualmente podemos concluir que, dado que se obtuviese una muestra de 100 unidades del conjunto de datos, es muy probable que el valor de la media calculado se encuentre entre 62 y 75. Esta conclusión podrá ser extendida al cálculo de cualquier media muestral y tomar valores determinados.\nEl proceso por el cual se llega a dichos valores se denomina estimación y es lo que se verá en el próximo capítulo."
  },
  {
    "objectID": "08 - estimacion puntual y por intervalos.html",
    "href": "08 - estimacion puntual y por intervalos.html",
    "title": "Estimación puntual y por intervalos de confianza",
    "section": "",
    "text": "Lectura\n\nEstimación y parámetros (Estadística para Todos: pág. 55)\nVariabilidad entre muestra y muestra (Estadística para Todos: pág. 58 a 63)\nTeorema Central del Límite (Estadística para Todos: pág. 200 a 207)\nDistribuciones de muestreo de estadísticas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 218 a 238)\nEstimación por intervalos (Estadística para Todos: pág. 214 a 228)\nEstimación e intervalos de confianza (Estadística Aplicada a los Negocios y la Economía: pág. 297 a 315)\nPropiedades deseables de los estimadores puntuales (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 251 a 261)\nConfidence Intervales (STAT 500 | Applied Statistics: lesson 5)\n\n\n\nActividades\n\nConsidere el dataset Pokemon y asuma que es el listado completo de todos los pokemones (es decir, la población). Use los parámetros obtenidos previamente y vuelva a calcular estimadores (utilizando las fórmulas adecuadas para cada caso, vistas en esta sección) pero realizando muestras aleatorias de manera sucesiva. Realice este mismo procedimiento con muestras de diferente tamaño. Grafique la distribución de los estimadores y saque conclusiones.\nRetome lo realizado en la anterior clase y estime nuevamente los valores de media, varianza y proporción para las variables seleccionadas, utilizando diferentes niveles de confianza. Escriba una breve reseña con conclusiones acerca de los resultados obtenidos.\n\n\n\nResumen\nEstimar es aproximar. Siendo que no se dispone, por razones operacionales o conceptuales, de toda la información disponible, habrá inevitablemente ciertas características de las variables que serán desconocidas; éstas pueden ser cualquiera de las medidas de posición, dispersión o relación vistas previamente. El verdadero valor de éstas es lo que recibirá el nombre de parámetro.\nEl objetivo será justamente aproximarse a estos parámetros a través de diferentes fórmulas, que reciben el nombre de estimadores. Para cada caso, no hay único estimador posible, aunque hay algunos que cumplirán con ciertas propiedades que los hacen óptimos o al menos mejores en comparación a otros.\nPara llegar a un valor aproximado de los parámetros, a través de los estimadores, serán necesario datos. Aquí entra en juego el proceso de muestreo, la obtención de datos, necesariamente aleatoria, para que la estimación arroje resultados válidos. Mientras mayor y más representativa sea una muestra respecto de la población, los valores de los estimadores estarán cada vez más cerca de los valores de los parámetros, siempre siendo distintos en cada muestra pero con una variación menor a medida que el tamaño de muestra tienda al número completo de unidades de análisis (o al infinito).\nLa variación en el resultado de los estimadores jugará un rol fundamental en la gestión de la incertidumbre. Es habitual no definir un valor tal para el estimador, sino más bien un intervalo. Se podrá decir que dicho intervalo, con cierto grado de confianza, contendrá al verdadero valor del parámetro en una cantidad de veces determinada (cantidad de muestras o de experimentos).\nTodos los resultados obtenidos serán sujetos al supuesto de que la población (de unidades de anális) tiene determinada distribución. Al intentar obtener una estimación de la media, de la varianza o una proporción, se deberá tener en cuenta que justamente son parámetros utilizados en distribuciones particulares, algunas de las vistas anteriormente, como la normal o binomial. Eso, en cierta forma, facilitará los cálculos y dará mayor certidumbre respecto de las conclusiones; pero, en muchos casos, las distribuciones no son conocidas o bien no se intentará estimar algunos de estos parámetros. En ese caso, se estará frente a la necesidad de recurrir a métodos no paramétricos de estimación, los cuales se verán más adelante."
  },
  {
    "objectID": "09 - tests de hipotesis.html",
    "href": "09 - tests de hipotesis.html",
    "title": "Tests de hipótesis",
    "section": "",
    "text": "Lectura\n\nDecisiones en el campo de la estadística (Estadística para Todos: pág. 231 a 241)\nPruebas de hipótesis de una muestra (Estadística Aplicada a los Negocios y la Economía: pág. 333 a 346)\nPruebas de hipótesis estadísticas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 303 a 346)\nHypothesis Testing for One-Sample Proportion (STAT 500 | Applied Statistics: lesson 6a)\nHypothesis Testing for One-Sample Mean (STAT 500 | Applied Statistics: lesson 6b)\n\n\n\nActividades\n\nConsidere el dataset Breast Cancer Data. Asuma que es resultado de un muestreo aleatorio, llevado a cabo en el marco de una investigación, y responda/realice lo siguiente:\n\nDefina población y unidad de análisis.\nDetalle las posibles características del procedimiento de muestreo llevado adelante.\nRealice un breve análisis exploratorio.\nPlantee al menos 3 preguntas relevantes para la investigación y que puedan ser respondidas a partir de dichos datos.\nA partir de las preguntas, defina las pruebas de hipótesis correspondientes y calcule los estimadores necesarios.\n\n(ENTREGA OBLIGATORIA) Vuelque todo lo anterior en un informe (máx. 10 páginas). Incluya:\n\nUn resumen del trabajo realizado a modo de introducción, considerando supuestos, planteamientos y resultados.\nUna sección con el análisis de los datos y procedimientos correspondientes. Explique y justifique cada paso.\nUn apartado final donde exponga conclusiones acerca de su trabajo, formule interrogantes acerca de la validez de sus conclusiones y presente posibles caminos de acción para subsanar/mejorar/ampliar la investigación.\n\n\n\n\nResumen\nPartiendo de la base que se calculó determinado estimador (es decir, se obtiene una aproximación de alguna característica de una variable), muchas veces lo que se busca es contrastar el resultado con alguna suposición acerca del valor real o quizás contra un valor dado por cierto. Esa puesta a prueba de lo conocido, recibirá el nombre de test de hipótesis. El test de hipótesis, haciendo uso de la evidencia (datos) relevada, buscará rechazar (o no) lo que se considera verdadero hasta el momento. Cabe señalar que justamente así funciona la ciencia en términos conceptuales, por tanto, los test de hipótesis son la herramienta por excelencia para la investigación científica.\nAsí como el cálculo de los estimadores, el resultado de un test de hipótesis dependerá de la calidad de los datos obtenidos, de la representatividad de la muestra. Esta obtención de datos debe ser aleatoria, al menos en algún punto del procedimiento general, y estará sujeta además a la suposición o no de que la variable (sobre la cual se testea) tenga determinada distribución. De no ser así, y al igual que con la estimación, se recurrirá a métodos no paramétricos.\nPor otro lado, las pruebas estadísticas (otra denominación para los tests) tendrán diferentes variaciones según se cuente con una o más muestras, una o más variables, variables independientes o apareadas (por ejemplo, la misma variable en dos momentos de tiempo diferentes), dispersión conocida o desconocida, entre otras cuestiones. Es una batería realmente extensa que dificilmente se pueda memorizar, con lo cual es recomendable más bien asimilar el reconomiento de escenarios frente a los cuales se puede aplicar un test y luego indagar en cuál sería el más apropiado para el mismo. En ese sentido, es una aproximación similar a la de la visualización de datos, pero para la cual se deberá ser mucho más precabido ya que, siendo una herramienta crítica para la obtención de conclusiones, puede conducir a acciones incorrectamente fundamentadas."
  },
  {
    "objectID": "10 - estadistica no parametrica.html",
    "href": "10 - estadistica no parametrica.html",
    "title": "Estadística NO paramétrica",
    "section": "",
    "text": "Lectura\n\nPrueba sobre la mediana\n\nPrueba de los signos (Estadística Aplicada a los Negocios y la Economía: pág. 681 a 685)\nInference for the Population Median (STAT 500 | Applied Statistics: lesson 11.1)\nDistribution-Free Confidence Intervals for Percentiles (STAT 415 | Introduction to Mathematical Statistics: lesson 19)\n\n\n\nPruebas sobre distribuciones (bondad de ajuste / independencia / aleatoriedad)\n\nMétodos no paramétricos: pruebas de bondad de ajuste (Estadística Aplicada a los Negocios y la Economía: pág. 648 a 658)\nPrueba de hipótesis de que la distribución de datos proviene de una población normal (Estadística Aplicada a los Negocios y la Economía: pág. 659 a 664)\nAnálisis de tablas de contingencia (Estadística Aplicada a los Negocios y la Economía: pág. 667 a 670)\nLa prueba de bondad de ajuste chi-cuadrada (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 363 a 368)\nLa estadística de Kolmogorov-Smirnov (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 368 a 370)\nChi-Square Test for Independence (STAT 500 | Applied Statistics: lesson 8)\nChi-Square Goodness-of-Fit Tests (STAT 415 | Introduction to Mathematical Statistics: lesson 16)\nContingency Tables (STAT 415 | Introduction to Mathematical Statistics: lesson 17)\nThe Wilcoxon Tests (STAT 415 | Introduction to Mathematical Statistics: lesson 20)\nRun Test and Test for Randomness (STAT 415 | Introduction to Mathematical Statistics: lesson 21)\nKolmogorov-Smirnov Goodness-of-Fit Test (STAT 415 | Introduction to Mathematical Statistics: lesson 22)\n\n\n\nBootstrapping\n\nIntroduction to Bootstrapping (STAT 500 | Applied Statistics: lesson 11.2)\nBootstrapping Main Ideas!!! (StatQuest YouTube Channel)\n\n\n\n\nActividades\n\nConsidere el dataset Wendy’s Menu Nutrition Data. Asuma que es resultado de un muestreo aleatorio de productos y responda/realice lo siguiente:\n\nRealice la prueba de Kolmogorov-Smirnov sobre todas las variables y concluya cuáles de ellas no tienen distribución normal. Complemente con métodos gráficos si considera necesario.\nUtilizando el método de boostrapping, lleve adelante la estimación de media, mediana y desviación estándar para todas aquellas variables.\nSe ha afirmado que el 50% de los productos tiene más de 800 calorías. ¿Está usted de acuerdo? Evalúe y compare distintos métodos para llegar a una conclusión.\nSe ha afirmado que la valoración de los productos está relacionada a su cantidad de grasa. ¿Está usted de acuerdo? Tenga en cuenta adecuar los datos para poder realizar el correspondiente análisis.\n\n\n\n\nResumen\nPrimero y principal, hay muchas hipótesis que escapan del simple cuestionamiento de si un valor (estimado) es igual a otro, como se vió en el capítulo anterior. Por ejemplo, se puede directamente querer comprobar (rechazar o no) si una variable tiene o no cierta distribución, sin necesidad del conocimiento o estimación de sus parámetros. Bajo un concepto similar, se puede pretender corroborar (rechazar o no) si una variable es independiente de otra o si es incluso aleatoria, lo cuales son supuestos en gran parte de los análisis. Siendo así, que no se busca la obtención ni se recurre al uso o definición de algún parámetro de interés (al menos no uno asociado a una distribución conocida, como la mediana), surge una batería de métodos / tests / pruebas denominados no parámetricos.\nAsí como con los tests parámetricos, la cantidad disponible de situaciones en los cuales pueden aplicarse son incontables, y es preferible disponer de la habilidad de reconocer cuándo o no es apropiado su uso y bajo qué condiciones. Cabe señalar que este tipo de pruebas, muy propias del ámbito estadístico, suelen hallarse implementados de manera más completa o detallada en el lenguaje R, y es recomendable su uso siendo que hay mayor cantidad de documentación e información académica disponible.\nEn los últimos años, de la mano de la ciencia de datos, se ha popularizado bastante el uso de un método no paramétrico en particular: boostrap. En esencia, implica el remuestreo (es decir, la muestra de una muestra) con reposición, de tal manera de generar un conjunto de muestras mayor al que se dispone (posiblemente una sola). Para cada muestra, se obtiente un estimador (de la manera habitual que ya se ha visto) y la distribución de valores estimados resultantes justamente pasa a ser el valor agregado de este método, que de otra manera se obtendría mediante muestras reales (operacionalmente imposible en la mayoría de los casos). Es importante señalar que, bajo este procedimiento, sumamente simple, se puede estimar cualquier parámetro o característica de interés de una variable. Solamente es necesario una muestra, la cual pasa a ser crucial en términos de calidad y representatividad, y soporte para el remuestreo (una simulación computacional). El bootstrapping se encuentra implementado en python / R, muy bien documentado y con muchos ejemplos, dada su uso frecuente hoy día."
  },
  {
    "objectID": "11 - tecnicas de muestreo.html",
    "href": "11 - tecnicas de muestreo.html",
    "title": "Técnicas de muestreo",
    "section": "",
    "text": "Lectura\n\nMétodos de muestreo (Estadística Aplicada a los Negocios y la Economía: pág. 266 a 271)\nElección del tamaño adecuado de una muestra (Estadística Aplicada a los Negocios y la Economía: pág. 316 a 321)\nEstimating Population Mean and Total under SRS (STAT 506 | Sampling Theory and Methods: lesson 1)\nConfidence Intervals and Sample Size (STAT 506 | Sampling Theory and Methods: lesson 2)\nStratified Sampling (STAT 506 | Sampling Theory and Methods: lesson 6)\nCluster and Systematic Sampling (STAT 506 | Sampling Theory and Methods: lessons 7 y 8)\nMulti-Stage Designs (STAT 506 | Sampling Theory and Methods: lesson 9)\n\n\n\nActividades\n\nConsidere el dataset LinkedIn Freelancer Survey Results. Señale algunos aspectos claves a considerar en la realización de dicha encuesta (¿acaso podría implementarse un muestreo estratificado, por conglomerados o sistemático?) y haga una estimación del tamaño adecuado para la misma.\nConsidere el dataset London Bike Sharing Dataset. Asuma que es el total de registros por hora de viajes en bicicleta hasta el día de la fecha. Realice un análisis exploratorio de la variable cnt (total de viajes), principalmente calculando la media y desvío estándar. Luego, implemente un muestreo sistemático de los datos, estime los paramétros correspondientes y compare. Realice la comparación considerando temporada (season) y fines de semana (weekend).\n\n\n\nResumen\nLas situaciones reales para la obtención de datos dificilmente permiten que un muestreo aleatorio simple sea llevado adelante. Por ejemplo, cuestiones como los costos operacionales debido a la extensión de territorio son comunes en cualquier estudio de estadísticas sociales. Es así que han surgido diferentes técnicas de muestreo y estimadores asociadas a las mismas, de tal manera que la aproximación a los parámetros reales de la población sea la mejor (que es el objetivo de cualquier estimador).\nDicho eso, hay una consideración inicial muy importante al comenzar el muestreo: el tamaño de la muestra. Si es muy pequeño, la representatividad podría no ser adecuada (con todo lo que ello implica y que se ha señalado anteriormente); si es muy grande, es esencialmente gasto innecesario de recursos. Por tanto, determinar un adecuado tamaño de muestra es el puntapié para un buen análisis de datos, que estará atado luego a la técnica con la cual se realiza el muestreo.\nEn ese sentido, hay 2 que recursos que destacan: el uso de estratos y el uso de conglomerados.\nLos primeros (estratos) están pensados para estudiar regiones o agrupamientos de interés para la investigación, obteniendo estadísticas no sólo a nivel general sino al interior de cada uno. Es una división que puede realizarse sobre la población (el conjunto total de unidades de análisis) antes o después del muestreo (post estratificación), con la condición de estudiar el conjunto completo de estratos, ya que posiblemente cada estrato sea una representación parcial de la población. Sobre cada estrato se realiza una muestra aleatoria que luego se estudia en conjunto con el resto. Un ejemplo de estratos pueden ser los barrios de una ciudad.\nLos segundos (conglomerados) son agrupamientos de unidades de análisis que suelen presentarse naturalmente juntos y que cuentan con la particularidad de ser representaciones de la población por sí mismos, así como también de facilitar el estudio / muestreo. Los conglomerados no se muestrean, sino que se relevan todas las unidades al interior de los mismos. La unidad de muestreo es entonces el conglomerado, siendo éstos los que se seleccionan, habitualmente de forma sistemática. Un ejemplo de conglomerados podrían ser los pasajeros de un vuelo."
  },
  {
    "objectID": "12 - anova.html",
    "href": "12 - anova.html",
    "title": "ANOVA: Análisis de la varianza",
    "section": "",
    "text": "Lectura\n\nAnálisis de la varianza (Estadística Aplicada a los Negocios y la Economía: pág. 410 a 440)\nIntroduction to ANOVA (STAT 500 | Applied Statistics: lesson 10)\n\n\n\nActividades\n\nConsidere el dataset CNC turning Experiment. Identifique aquellos factores para los cuales la media de la fuerza (force), en al menos un grupo, sea significativamente diferente. Evalué en todos los casos los supuestos del análisis. Si para un grupo de factores encuentra diferencias, determine entre cuáles. Concluya cuáles factores son los que determinan la mayor fuerza posible alcanzada.\nLleve adelante una transformación de los datos tal que las réplicas de cada corrida (run_id) pasen a conformar las columnas r1 a r6 y además calcule la media y desvío estándar asociadas a las mismas."
  },
  {
    "objectID": "13 - regresion lineal.html",
    "href": "13 - regresion lineal.html",
    "title": "Regresión Lineal",
    "section": "",
    "text": "Lectura\n\nRelación entre variables (Estadística para Todos: pág. 168 a 196)\nEl estudio de la relación entre variables (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 105 a 130)\nRegresión lineal y correlación (Estadística Aplicada a los Negocios y la Economía: pág. 461 a 494)\nAnálisis de regresión: el modelo lineal simple (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 443 a 478)\nFundamentals of Data Visualization (Part I: Cap. 12)\nLinear Regression Foundations (STAT 500 | Applied Statistics: lesson 9)\nIntroduction to Modeling Libraries in Python (Python for Data Analysis: cap. 12)\nIn Depth: Linear Regression (Python Data Science Handbook)\n\n\n\nActividades"
  },
  {
    "objectID": "14 - regresion logistica.html",
    "href": "14 - regresion logistica.html",
    "title": "Regresión Logística",
    "section": "",
    "text": "Lectura\n\n\nActividades"
  },
  {
    "objectID": "15 - knn y cart.html",
    "href": "15 - knn y cart.html",
    "title": "KNN y CART",
    "section": "",
    "text": "Lectura\n\n\nActividades"
  }
]