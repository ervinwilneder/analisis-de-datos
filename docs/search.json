[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de datos",
    "section": "",
    "text": "Prefacio\nEl presente libro es un compilado de lecturas y actividades sobre diferentes temas que son considerados fundamentales para el análisis de datos. Cada capítulo corresponde a un tema e incluye un resumen con ideas y conclusiones a destacar. Donde corresponda, se dispone además un apartado con implementaciones en diversos lenguajes/herramientas, principalmente utilizando Python.\nEste material fue desarrollado exclusivamente para las materias Análisis de Datos (I y II) y Python de la Diplomatura Universitaria en Programación de la UTN HAEDO, dictada inicialmente en 2023.\nPor recomendaciones, correcciones u otras propuestas, por favor escribir a ervinwilneder@gmail.com o a través de mi LinkedIn."
  },
  {
    "objectID": "01 - conceptos y definiciones.html#lectura",
    "href": "01 - conceptos y definiciones.html#lectura",
    "title": "Conceptos y definiciones",
    "section": "Lectura",
    "text": "Lectura\n\nhttps://aws.amazon.com/es/what-is/data-analytics/\nhttps://www.intel.la/content/www/xl/es/artificial-intelligence/what-is-data-analytics.html\nhttps://www.oracle.com/business-analytics/data-analytics/"
  },
  {
    "objectID": "01 - conceptos y definiciones.html#actividades",
    "href": "01 - conceptos y definiciones.html#actividades",
    "title": "Conceptos y definiciones",
    "section": "Actividades",
    "text": "Actividades\n\n¿Qué es Data Analytics? ¿Considera que es lo mismo que Análisis de Datos?\n¿En qué se diferencia de Big Data Analytics?\n¿Cuáles son los pasos a seguir en un proceso de análisis de datos?\nMencione al menos 3 herramientas vinculadas al análisis de datos\n¿Qué tipo de análisis de datos se pueden hacer?\nRelacione la anterior respuesta con los siguientes conceptos: raw data, information, knowledge, insight, actionable insight, decision-making, model\nDiferencie brevemente Machine Learning, Data Mining, Data Science, Business Intelligence, Data Analysis y Data Analytics"
  },
  {
    "objectID": "01 - conceptos y definiciones.html#resumen",
    "href": "01 - conceptos y definiciones.html#resumen",
    "title": "Conceptos y definiciones",
    "section": "Resumen",
    "text": "Resumen\nEl análisis de datos es una actividad que se encuentra integrada a otras como ingeniería y ciencia de datos, compartiendo conceptos y herramientas, y habitualmente superponiéndose en el día a día de muchas organizaciones. Por esto mismo, como profesional del área, es importante reconocer cuáles son las incumbencias del análisis de datos y cómo se relaciona con el resto del ciclo de vida de los datos.\nSurge entonces la necesidad de definir y comprender conceptos fundamentales para lograr esa distinción. Como suele suceder en otros rubros, varios de estos conceptos son asimilados del inglés y se utilizan diariamente de esta manera, lo que contribuye a la confunsión en algunos casos. Además, las definiciones no siempre son unísonas: dependen de cada autor y evolucionan en el tiempo. Implica así una capacitación, actualización y reflexión constante.\nComo actividad, el análisis de datos requiere principalmente conocimientos de estadística y de dominio (o de negocio), de tal manera de poder trabajar los datos, obtener conclusiones y traducir esas conclusiones en recomendaciones o acciones (o simplemente producir información / conocimiento para su posterior tratamiento).\nEl rol no necesariamente implica el uso de lenguajes de programación, aunque (en el estadío actual del área) suele ser altamente recomendable adquirir su uso. De no contar con ello, es esencial la experiencia en al menos una herramienta de visualización (Tableau, PowerBI, Looker) y uso avanzado de planillas de cálculo (Excel, Google Sheets).\nRetomando lo dicho, es habitual también la necesidad de un cierto grado de transformación de los datos. En ese aspecto, Python, R y SQL son herramientas de gran ayuda, que desde ya pueden utilizarse más adelante en el análisis como tal. De hecho, su uso se ha consolidado cada vez más de forma mandatoria en términos de poder trabajar con grandes volúmenes de datos y elaborar modelos complejos, inviables de implementar con otras herramientas como las mencionadas anteriormente.\nPosteriormente al análisis, la presentación de los mismos es sumamente importante, y es incluso un diferencial. En ese sentido, un analista de datos completo cuenta con la capacidad para la elaboración de informes y visualizaciones, pudiendo abstraerse de la parte técnica y mostrar efectivamente y de manera sencilla los resultados de su análisis.\nRecapitulando, el análisis de datos pasa a ser hoy un conjunto de conocimientos y técnicas que, con las herramientas adecuadas y enmarcado correctamente en un proceso de analítica de datos, permite la toma de decisiones dentro de las organizaciones a través de la exploración e inferencia de los datos."
  },
  {
    "objectID": "02 - entorno de trabajo.html#lectura",
    "href": "02 - entorno de trabajo.html#lectura",
    "title": "Entorno de trabajo",
    "section": "Lectura",
    "text": "Lectura\n\nPlanillas de cálculo\n\nTareas básicas en Excel (Microsoft Support)\nGoogle Sheets (Google Cloud Skills Boost)\n\nJupyter Notebooks y otras plataformas similares\n\nPython Language Basics, IPython, and Jupyter Notebooks (Python for Data Analysis: cap. 2.1 y 2.2)\nhttps://docs.jupyter.org/en/latest/use/using.html\nhttps://colab.research.google.com/\nhttps://posit.co/\nhttps://code.visualstudio.com/\nhttps://deepnote.com/\nhttps://www.datacamp.com/workspace\nhttps://observablehq.com/\n\nHerramientas de visualización y consulta de datos\n\nhttps://powerbi.microsoft.com/en-au/\nhttps://www.tableau.com/\nhttps://www.metabase.com/\n\nSuites de analítica de datos\n\nhttps://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform\nhttps://aws.amazon.com/es/sagemaker/\nhttps://www.paperspace.com/gradient\nhttps://learn.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning\n\nVersionado\n\nhttps://git-scm.com/\nhttps://docs.github.com/en/get-started/quickstart/hello-world\nhttps://dvc.org/\n\nGestores de ambientes\n\nhttps://www.anaconda.com/\n\nConteinerización\n\nhttps://www.docker.com/\nhttps://www.digitalocean.com/community/tutorials/el-ecosistema-de-docker-una-vision-general-de-la-contenerizacion-es\nhttps://aws.amazon.com/es/docker/"
  },
  {
    "objectID": "02 - entorno de trabajo.html#actividades",
    "href": "02 - entorno de trabajo.html#actividades",
    "title": "Entorno de trabajo",
    "section": "Actividades",
    "text": "Actividades\n\nInstale Docker Desktop (https://www.docker.com/products/docker-desktop/) en su computadora. Luego clone el siguiente repositorio https://github.com/ervinwilneder-untref/statistics-playground y levante el servicio de JupyterLab."
  },
  {
    "objectID": "02 - entorno de trabajo.html#resumen",
    "href": "02 - entorno de trabajo.html#resumen",
    "title": "Entorno de trabajo",
    "section": "Resumen",
    "text": "Resumen\nUno de los puntos de fricción más habituales para quien comienza con análisis de datos es la abrumadora cantidad de herramientas y entornos de trabajo disponibles. Incluso habiéndose decidido, suele no ser tarea sencilla su configuración y la curva de aprendizaje es lenta, particularmente para el caso de aquellas personas sin un background en programación. Sin embargo, junto con el claro entendimiento de la teoría y práctica, el buen uso de las herramientas es un factor clave en el éxito profesional.\nPor lo tanto, antes de comenzar a realizar tareas propias de un/a analista de datos, se hace necesario transitar el panorama de plataformas destacadas, en cada una de las distintas cuestiones relacionadas a un correcto desarrollo/trabajo.\nComenzando con las planillas de cálculo, éstas son las herramientas con más trayectoria dentro del ámbito. Más allá de que hoy hay un sinfín de tareas complejas y volumen de datos que no pueden manejar, suele ser en el tramo final de un proyecto (la disponibilización de resultados para usuarios finales) donde están presentes, tanto por su sencillez así como también porque los mismos usuarios están acostumbrados a su uso.\nLuego están las notebooks, generalmente asociadas Jupyter, que son esencialmente documentos con la capacidad de volcar texto y código (python, r, etc.), con una dinámica de ejecución por celdas, haciendo de las mismas la herramientas por excelencia para lo/as científicos de datos. Su uso actualmente se ha ido extendiendo y prácticamente cualquier rol involucrado a datos comprende su uso y características, considerándose algo así como un estándar. Mediante la implementación de diferentes kernels, las notebooks integran varios lenguajes, aunque su uso suele estar más asociado a Python. Para aquellos que programan en R, el uso de RStudio (ahora llamado Posit) resulta más cómodo, aunque ahora se está volviendo una plataforma con soporte para más lenguajes y puede llegar a ser una excelente opción como entorno de trabajo.\nTambién hay otras herramientas que copian la misma dinámica de las notebooks y/o la extienden con nuevas funcionalidades. En ese sentido, el abanico de opciones es bastante amplio, destacándose principalemente Colab, por ser gratuita y con una capacidad de procesamiento generosa; o JupyterLab, que es una IDE que permite la gestión de notebooks, instalación de plugins y más funcionalidades. Ésta última de hecho es adoptada por varias empresas para montar sobre la misma servicios mucho más potentes y on-cloud como GCP, AWS y Azure, conformando suites de analítica de datos, en constante adopción por las empresas y organizaciones de hoy.\nA lo anterior, debe sumarme las herramientas exclusivamente pensadas para visualización de datos, que comienzan actualmente a integrar capacidades de consulta (antes sólo atribuibles a un cliente de bases de datos) y transformación de datos (más allá de las herramientas de ETL propiamente dichas) dentro del mismo entorno.\nComo se habrá podido concluir, este primer vistazo ya trajo consigo bastantes nuevas cosas para investigar y comprender. Lo importante, a fin de cuentas, es tener suficiente flexibilidad para adoptar sobre la marcha las herramientas que se dispongan en el trabajo o proyectos particulares. No se tiene que dejar pasar que habrá ciertas funcionalidades comunes a todas aquellas así como también ciertas propiedades deseables (no sólo para las herramientas, sino para cualquier análisis de datos en sí): transparencia, replicabilidad, interoperabilidad y escalabilidad.\nTeniendo en cuenta ello, entran en juego otra batería de herramientas importantes. Se destacan principalmente: Git para la transparencia (versionado y experimentación) y Docker / Anaconda para la replicabilidad (definición y gestión de dependencias). Considerar que no vale de nada un desarrollo que no pueda ser replicado o utilizado correctamente por otros colaboradores o incluso usuarios finales, en la era del trabajo colaborativo estas herramientas son prácticamente mandatorias."
  },
  {
    "objectID": "03 - lenguajes de programación.html#lectura",
    "href": "03 - lenguajes de programación.html#lectura",
    "title": "Lenguajes de programación",
    "section": "Lectura",
    "text": "Lectura\n\nPython\n\nPython for Data Analysis\nCS50’s Introduction to Programming with Python\nIntro a Python (Repositorio de cursos de Python del Instituto Humai)\nProgramación en Python (Repositorio de curso de Python de la UNSAM)\nReal Python\nAprendiendo Python | Curso de Python 2020 desde cero…\n\nOtros\n\nR for Data Science\nLearnR4Free\nLearning SQL\nBigQuery SQL Syntax Documentation\nJulia Data Science\nObservable Tutorials (Javascript)"
  },
  {
    "objectID": "03 - lenguajes de programación.html#actividades",
    "href": "03 - lenguajes de programación.html#actividades",
    "title": "Lenguajes de programación",
    "section": "Actividades",
    "text": "Actividades\n\nUtilice el entorno de trabajo previamente instalado y comience a realizar operaciones básicas en cada lenguaje. Esta actividad no tiene una propuesta puntual, pero se sugiere seleccionar algún dataset y resolver cómo llevar adelante su ingesta en cada herramienta disponible."
  },
  {
    "objectID": "03 - lenguajes de programación.html#resumen",
    "href": "03 - lenguajes de programación.html#resumen",
    "title": "Lenguajes de programación",
    "section": "Resumen",
    "text": "Resumen\nEntre las habilidades requeridas para un analista de datos, se destaca en la actualidad el uso de lenguajes de programación. Como se mencionó anteriormente, hasta cierto punto, este rol puede prescindir de los mismos. De hecho, muchas de las actividades que se verán de aquí en adelante pueden resolverse sin recurrir a aquellos. Sin embargo, en pos del crecimiento profesional y el contexto relativo al volumen y complejidad de los datos, los lenguajes de programación resultan esenciales en términos prácticos. Sobre todo si se retoma lo comentado acerca de las cualidades deseadas de un bien realizado análisis de datos (recordar: transparencia, replicabilidad, interoperabilidad y escalabilidad), las cuales los lenguajes pueden abordar sin problema.\nEn compañia de este libro, se priorizará el uso de Python, aunque puede ser utilizado cualquiera. Sucede en la práctica que algunos lenguajes son referidos para ciertas tareas más que otros. Python es bastante generalista, sirve tanto para análisis de datos, para ingeniería y ciencia de datos (las librerías referentes en machine learning están realizadas en python), para usarse como backend en desarrollos web, web scraping, entre otras tareas más. Por otro lado, R se relaciona más al ámbito de la investigación y ciencia, con sus orígenes en la estadística computacional. Julia es un lenguaje incipiente, con un repertorio muy interesante de desarrollos y con la intención de unir lo mejor de Python y R. Y finalmente, Javascript, que al igual que python se utiliza en diferentes ámbitos, incluso más, pero en análisis de datos se puede considerar una muy buena herramienta especialmente para visualización de datos.\nSQL merece mención aparte porque, en lo respecta a uso de bases de datos (para el almacenamiento y consulta de datos), es la norma. Es decir, es inevitable su uso y cualquier búsqueda laboral para roles de análisis de datos es probable que lo señalen como un requisito excluyente. Para este lenguaje pueden encontrarse diferentes implementaciones según la empresa/organización que promueva su propia base de datos. Suelen todos tener una base común y luego un set de funciones / syntaxis particular, que se adquiere rápidamente con la práctica."
  },
  {
    "objectID": "04 - datos unidades y variables.html#lectura",
    "href": "04 - datos unidades y variables.html#lectura",
    "title": "Datos, unidades y variables de análisis",
    "section": "Lectura",
    "text": "Lectura\n\nLos Datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 13 a 15)\nLas Variables (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 15 a 17)\nLa Primera Organización de los Datos: la matriz de datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 25 a 27)\nTidy Data (Journal of Statistical Software: cap. 1, 2 y 3)\nTipos de variables (Estadística Aplicada a los Negocios y la Economía: pág. 8 y 9)\nVariables aleatorias (Estadística Aplicada a los Negocios y la Economía: pág. 189 y 190)\nTypes of Data (STAT 414 | Introduction to Probability Theory: lesson 1.4)\nEl concepto de variable aleatoria (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 52 y 53)"
  },
  {
    "objectID": "04 - datos unidades y variables.html#actividades",
    "href": "04 - datos unidades y variables.html#actividades",
    "title": "Datos, unidades y variables de análisis",
    "section": "Actividades",
    "text": "Actividades\n\nBusque y analice algún artículo que hable acerca de la diferencia entre datos, información y conocimiento. Provea un ejemplo propio que muestre dicha diferencia.\nConsidere el dataset Top Trends on TikTok & YoutubeShorts. Determine la unidad de análisis, las variables y su tipo. ¿Es alguna de ellas aleatoria?"
  },
  {
    "objectID": "04 - datos unidades y variables.html#resumen",
    "href": "04 - datos unidades y variables.html#resumen",
    "title": "Datos, unidades y variables de análisis",
    "section": "Resumen",
    "text": "Resumen\nLos datos son el insumo básico para el análisis de datos. La mínima unidad de información, la materia prima a partir de la cual se produce conocimiento y se pueden tomar acciones basadas en evidencia con cierto grado medible de incertidumbre.\nEs adecuado pensar a un dato como aquello que puede colocarse dentro de una celda (de una planilla); esto es, un número, una letra, una palabra, un verdadero o falso. El hecho de considerar a algo como un dato dependerá del análisis mismo, porque ese dato surgirá como resultado del entrecruzamiento de 2 conceptos sumamente importantes: la unidad de análisis y la variable.\nLa unidad de análisis es el objeto o sujeto sobre el cual se lleva adelante el análisis. La variable es la característica de interés de ese objeto o sujeto, sobre el cual se observará o medirá un dato. La primera suele identificarse como cada una de las filas (de una planilla), la segunda como las columnas (ver imagen); aunque no siempre tiene por qué ser así. Entonces, la secuencia inicial de cualquier análisis de datos será identificar la unidad de análisis, la o las variables de estudio y obtener de estas variables uno o más datos.\n\n\n\n\nvariable 1\nvariable 2\nvariable 3\n\n\n\n\nunidad 1\ndato\ndato\ndato\n\n\nunidad 2\ndato\ndato\ndato\n\n\nunidad 3\ndato\ndato\ndato\n\n\n\nLas variables pueden ser de 2 tipos: cuantitativas o cualitativas. Las primeras, asimismo, pueden ser continuas o discretas, y las segundas, ordinales o nominales. Lo importante de reconocer qué tipo de variable se tiene a la mano está en el hecho de anticipar las técnicas y limitaciones o no para cada caso. Las variables más flexibles y potentes son las cuantitativas continuas porque las mismas pueden discretizarse, ordenarse o transformarse en cualitativas, no así al revés.\nPor otra parte, la clasificación de variables puede implicar no sólo su tipo, sino también si es resultado o no de un proceso aleatorio. Es decir, si es producto del azar en algún punto de su obtención. Esto traerá apareado un cambio completo en el modo de analizar a la variable, así como también de las conclusiones que pueden extraerse.\nRecapitulando, como analista de datos, es sumamente importante participar o tener conocimiento pleno en la definición exhaustiva de la unidad de análisis, del recocimiento de las variables y de la obtención y organización de los datos, ya que estos 3 puntos son la base sobre la cual se construye por completo cualquier análisis."
  },
  {
    "objectID": "05 - tablas de frecuencias.html#lectura",
    "href": "05 - tablas de frecuencias.html#lectura",
    "title": "Tablas de frecuencias",
    "section": "Lectura",
    "text": "Lectura\n\nConstrucción de una tabla de frecuencias (Estadística Aplicada a los Negocios y la Economía: pág. 23 y 24)\nConstrucción de distribuciones de frecuencias: datos cuantitativos (Estadística Aplicada a los Negocios y la Economía: pág. 29 a 34)\nProblema de Trabajo e Investigación Estadística (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 12 y 13)\nEl Análisis de la Matriz de Datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 28)\nLas Distribuciones de Frecuencias en el Análisis Univariado (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 29 a 45)"
  },
  {
    "objectID": "05 - tablas de frecuencias.html#actividades",
    "href": "05 - tablas de frecuencias.html#actividades",
    "title": "Tablas de frecuencias",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el set de datos Top Trends on TikTok & YoutubeShorts. Elabore tablas de frecuencias absolutas, relativas y acumuladas (si corresponde) para todas las variables.\nReleve el uso e implementación de tablas pivote en diferentes softwares/plataformas (ej: Excel, Google Sheets, Python, R, entre otros). Utilice alguno de los ejemplos sugeridos para realizar las anteriores tablas.\nElabore al menos 3 preguntas de negocio que podrían ser respondidas con las anteriores tablas."
  },
  {
    "objectID": "05 - tablas de frecuencias.html#resumen",
    "href": "05 - tablas de frecuencias.html#resumen",
    "title": "Tablas de frecuencias",
    "section": "Resumen",
    "text": "Resumen\nLa forma más básica de analizar / resumir un conjunto de datos es una tabla de frecuencias. Implica la operación de conteo de unidades de análisis o de sus variables, agrupadas por algún criterio, conformando categorías o clases excluyentes (sin solapamiento). En las variables cualtitativas, suele ser bastante directo, siendo que las categorías se encuentran naturalmente identificadas; en las cuantitativas dependerá del análisis, pudiendo agrupar por intervalos del mismo o diferente tamaño, según se necesite.\nEl resultado puntual del conteo para cada clase o intervalo se denomina frecuencia y el conjunto de éstas se conoce como distribución de frecuencias. La frecuencia puede ser absoluta o relativa (si es que se divide por el total de unidades o total de la variable), dando una mejor idea de cuán relevante es cierta clase respecto del total y además permitiendo comparar con otras variables. La suma de la columna con las frecuencias absolutas debe dar siempre como resultado el conteo de unidades total, mientras que la relativa debe dar siempre 1.\nAsí, la tabla de frecuencias puede construirse inicialmente de la siguiente manera: una columna con las clases / categorías / intervalos, una columna con las frecuencias absolutas y otra columna con las relativas. Éstas 2 últimas suelen acompañarse de una fila con el total, de tal manera de verificar lo anteriormente dicho. Por sobre esta configuración básica, se puede agregar una columna con el punto medio de clase (habitualmente la diferencia entre valor máximo y mínimo de un intervalo, dividido 2) o punto máximo, lo cual ayuda a identificar rápidamente un intervalo sin recurrir a su definición por valores (ambos máximo y mínimo), y también siendo soporte para una posterior visualización. Adicionalmente, también puede utilizarse otra columna con el ancho de clase ó amplitud (diferencia entre valor máximo y mínimo).\nPara responder algunas preguntas del análisis puede llegar a ser necesario el cálculo además de frecuencias acumuladas (esto únicamente para variables susceptibles de ordenamiento). Esencialmente, una vez ordenada la tabla por algún criterio, se procede a la suma fila por fila de las frecuencias absolutas y/o relativas. Sobre estas nuevas columnas no tiene sentido realizar una suma, pero sí se debe constatar que el resultado de la última fila sumada dé el total de la frecuencia absoluta o relativa, según se haya utilizada una u otra.\nPudiendo realizarse de forma manual, en conjuntos grandes de datos, es posible recurrir al uso de tablas pivote o alguna implementación específica para tablas de frecuencia (ej: función FREQUENCY en Google Sheets).\nFinalizando lo referente a la construcción de la tabla de frecuencias, lo más importante a destacar es su flexibilidad y alcance frente a cualquier conjunto de datos. Se recomienda su implementación en cada análisis, para todas las variables, permitiendo posiblemente detectar problemáticas en las mismas o incluso sacar algunas primeras conclusiones. Además, serán necesarias para la realización de algunos tipos de análisis específicos vistos más adelante."
  },
  {
    "objectID": "05 - tablas de frecuencias.html#implementaciones",
    "href": "05 - tablas de frecuencias.html#implementaciones",
    "title": "Tablas de frecuencias",
    "section": "Implementaciones",
    "text": "Implementaciones"
  },
  {
    "objectID": "06 - descripcion de variables.html#lectura",
    "href": "06 - descripcion de variables.html#lectura",
    "title": "Descripción de variables",
    "section": "Lectura",
    "text": "Lectura\n\nMedidas de posición: promedio, mediana, cuantiles y moda\n\nMedidas Resumen (Estadística para Todos: pág. 120 a 125)\nLos valores que caracterizan al conjunto de datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 59 a 79)\nDescripción de datos (Estadística Aplicada a los Negocios y la Economía: pág. 57 a 75)\nMedidas numéricas descriptivas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 11 a 14)\n\nMedidas de dispersión: desviación estándar, coeficiente de variación y rango\n\nMedidas Resumen (Estadística para Todos: pág. 125 a 135)\nAnálisis de la variación y asimetría (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 85 a 97)\nDescripción de datos (Estadística Aplicada a los Negocios y la Economía: pág. 75 a 84)\nMedidas numéricas descriptivas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 15 a 22)\n\nMedidas de relación: razones, tasas y proporciones\n\nRazón, tasas y porcentajes (Estadística para Todos: pág. 15 a 19)"
  },
  {
    "objectID": "06 - descripcion de variables.html#actividades",
    "href": "06 - descripcion de variables.html#actividades",
    "title": "Descripción de variables",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Best Movie by Year Netflix. Calcule medidas de posición y dispersión para la variable score. Divida el set de datos a partir de algún criterio que considere de interés, calcule medidas de relación entre los grupos resultantes."
  },
  {
    "objectID": "06 - descripcion de variables.html#resumen",
    "href": "06 - descripcion de variables.html#resumen",
    "title": "Descripción de variables",
    "section": "Resumen",
    "text": "Resumen\nEn el proceso de caracterización / resumen de variables, el análisis exploratorio de datos, participan medidas de 3 cuestiones fundamentales: posición, dispersión y relación.\nLa posición esencialmente permite conocer dónde se encuentran la mayor parte de los datos y la dispersión, saber si los datos que conforman esa mayoría se encuentran cerca unos de otros. Cuando la dispersión aumenta, las medidas de posición realmente no dicen mucho (aunque parezca que sí); por esto mismo, siempre deben estar ambas presentes. Justamente, como la dispersión impacta sobre la posición, se cuenta con una batería de posibles medidas que son más robustas frente a una situación de gran dispersión ó dispersión particular de algunos datos. La más utilizada es la mediana, que a diferencia del promedio (media) logra mantenerse firme frente a valores extremos.\nLa dispersión así también puede ser engañosa, porque depende de las unidades en la que está medida. Por esto mismo, es recomendable el uso del coeficiente de variación, que es una relativización del desvío estándar frente al valor del promedio. Como su nombre lo indica, es un coeficiente y no tiene unidades. Un valor menor a 0.3 (30%) indica baja dispersión y, por ende, las medidas de posición serán confiables.\nAún así, se verá en el próximo capítulo, que aún dándose esta última situación, contar sólo con las medidas de posición no es 100% seguro. Siempre debe recurrise a la visualización para lograr entender el comportamiento de una variable o conjunto de datos, acompañada de una tabla de frecuencias.\nMás allá del análisis de una variable por si sola, también puede presentarse la situación en la que se quiere comparar con otra o incluso comparar consigo misma en otra circunstancia (o en otro momento). Para ello, existen medidas de relación, que son habituales en todo tipo de informes. No se menciona en este capítulo el coeficiente de correlación, que merece un desarrollo más profundo de su uso e implicancias, lo cual se verá más adelante."
  },
  {
    "objectID": "07 - visualizacion.html#lectura",
    "href": "07 - visualizacion.html#lectura",
    "title": "Visualización de datos",
    "section": "Lectura",
    "text": "Lectura\n\nVizualización de cantidades, proporciones y distribuciones\n\nFundamentals of Data Visualization (Part I: Cap. 2, 5, 6, 7 y 10) (complementar con el resto de la bibliografía habitual de ser necesario)\nVisualización (Laboratorio de Datos – 1er Cuatrimestre 2021: Clase 5)\nExploratory Visualizations (Feature Engineering and Selection: A Practical Approach for Predictive Models: cap. 4)\nData Visualization (Python for Everybody: cap. 17)\n\nConceptos básicos de storytelling y presentaciones efectivas\n\nFundamentals of Data Visualization (Part III: Cap. 28 y 29)\nData Visualization - A practical introduction (Cap. 1)"
  },
  {
    "objectID": "07 - visualizacion.html#actividades",
    "href": "07 - visualizacion.html#actividades",
    "title": "Visualización de datos",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset French Bakery Daily Sales. Realice visualizaciones que puedan responder a las siguientes preguntas:\n\n¿Cuál fue el total de baguettes tradicionales vendidas en el período Sep-2022?\n¿Cuánto representa la anterior cantidad respecto a todo tipo de baguettes?\n¿Ha habido cambios en esas proporciones respecto a Sep-2021?\n¿Cuál fue el día de semana y horario de mayor ganancia?\n¿Cuánto representa la venta de baguettes tradicionales en dicha ganancia?\n¿Cuál es la variación respecto a unidades vendidas para todo tipo de baguette a lo largo del año 2021?\n¿Cuál es el mayor incremento intermensual de baguettes tradicionales vendidas?\n¿En qué precio se encuentran la mayoría de los productos? ¿y el 80% de los productos?\n\n(ENTREGA OBLIGATORIA) Considere el dataset French Bakery Daily Sales. Asuma que usted es un analista de datos recientemente contratada/o por la panadería francesa. El dueño busca entender el potencial de venta de la baguette tradicional. Realice una presentación de negocio con las visualizaciones anteriores que considere relevantes (modifíque ó cree otras si considera necesario) con el objetivo de describir la situación histórica y actual de la venta de dicho producto. No utilice más de 5 gráficos/diapositivas, debe ser breve y comunicar sólo cuestiones claves. Extraiga una conclusión / recomendación para el dueño."
  },
  {
    "objectID": "07 - visualizacion.html#resumen",
    "href": "07 - visualizacion.html#resumen",
    "title": "Visualización de datos",
    "section": "Resumen",
    "text": "Resumen\nEn el área de análisis de datos, puede llegar a ser un rol aparte el de aquel que realiza visualizaciones. Es un área que implica no sólo conocimiento técnico en datos, sino en diseño. Sino se tiene esta formación puntual, muchas veces se adquiere con la experiencia, y aún así, frente a una presentación, el contenido y la forma dependerá de quién reciba la información. Lo recomendable es inicialmente partir de un diseño mínimo, replicable y tomar referencias de otras presentaciones. Complementar y mejorar luego en función de la asimilación de principios básicos del diseño, colores e incluso tendencias.\nEn cuanto a los datos, las visualizaciones estarán dadas por la cantidad de variables y su tipo. Suele ser buen hábito recurrir a una galería de visualizaciones para corrobar cuál gráfico encaja con lo que se tiene disponible o se quiere mostrar (ej: Python Graph Gallery). Siempre adecuar la visualización a la pregunta que debe responder y evitar la sobrecarga / repetición de datos. Acompañar con el correspondiente título, referencias y leyenda, así como eventualmente, si fuese necesario, con una breve y sencilla explicación coloquial.\nDicho esto, es posible realizar visualizaciones tanto con herramientas pensadas para tal fin (Tableau, PowerBI, Looker), otras más genéricas de presentaciòn (PowerPoint, Google Slides) o con Python / R / Javascript (a través de las librerías adecuadas). En ese sentido, lo recomendable es adoptar una en particular y profundizar en su uso."
  },
  {
    "objectID": "08 - origen y recoleccion.html#lectura",
    "href": "08 - origen y recoleccion.html#lectura",
    "title": "Origen y recolección de datos",
    "section": "Lectura",
    "text": "Lectura\n\nOrigen de los datos (Estadística para Todos: pág. 51 y 52)\nMuestreo (Estadística para Todos: pág. 29 a 35)\nFuentes de datos (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 18 a 20)\nMétodos de muestreo (Estadística Aplicada a los Negocios y la Economía: pág. 266 a 268)\nMuestras aleatorias (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 214 a 217)\nMétodos de recolección de datos para una investigación"
  },
  {
    "objectID": "08 - origen y recoleccion.html#actividades",
    "href": "08 - origen y recoleccion.html#actividades",
    "title": "Origen y recolección de datos",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Pokemon. Lleve adelante la descripción de 2 variables numéricas (cálculo de media y desvío estándar) y 2 categóricas (cálculo de alguna proporción de interés). Luego extraiga una serie de muestras bajo algún criterio que le resulte adecuado. Vuelva a calcular los valores anteriores, compare y extraiga conclusiones."
  },
  {
    "objectID": "08 - origen y recoleccion.html#resumen",
    "href": "08 - origen y recoleccion.html#resumen",
    "title": "Origen y recolección de datos",
    "section": "Resumen",
    "text": "Resumen\nEl origen y recolección de los datos determina las conclusiones que pueden extraerse de los mismos. Tener un censo (un relevamiento completo de las unidades de análisis) no será lo mismo que tener una muestra (un relevamiento parcial); la segunda lleva consigo la incertidumbre propia de no contar con todos los datos. Pero justamente será ese el escenario donde el análisis de datos estará más presente.\nUna muestra será habitualmente la opción más viable para conseguir datos. Incluso en la era del big data, por más datos que se tengan, suelen ser representaciones parciales de la realidad, de los procesos que los generan. La muestra es la regla, no la excepción.\nEs un procedimiento mediante el cual se extrae, de forma aleatoria, una serie de unidades de análisis de un conjunto mayor (finito o infinito) de unidades. En su versión más simple, consiste en extraerlas directamente, aunque ésta puede no ser la mejor forma de hacerlo. Más adelante se verán otras metodologías que, en esencia, buscan mejorar la representatividad de los datos obtenidos respecto del conjunto total, o lo que es igual, disminuir los errores debidos al muestreo mismo. La aleatoriedad juega aquí un rol fundamental, siendo el canal por el cual se busca eliminar todo otro tipo de errores debido a la subjetividad, mientras que además se debe evitar otros errores por cuestiones operacionales. Cabe señalar además que la muestra no es solamente una técnica aplicada a datos que son reales, sino que se pueden obtener a partir de una simulación o datos sintéticos, lo cual será útil para la experimentación.\nUn muestreo (de acá en adelante siempre entendido como aleatorio) dará como resultado variables necesariamente aleatorias; el objetivo será caracterizarlas, pero teniendo en cuenta su naturaleza azarosa. Porque cada vez que se tome una muestra, los resultados de un promedio o una desviación estándar (u otra medida) para determinada variable, serán distintos. Todas y cada una de las veces, siempre será distinto. Estos resultados en sí, pueden asimismo considerarse nuevas variables y también caracterizarse. Justamente este es uno de los grandes pilares del análisis de datos.\nRecapitulando, lo que será de interés de aquí en adelante, es la capacidad de lidiar con una muestra y obtener conclusiones de las variables a partir de la misma. En el siguiente capítulo, se hará una introducción a la teoría y conceptos de probabilidad, que serán de gran ayuda."
  },
  {
    "objectID": "09 - manipulacion de datos.html#lectura",
    "href": "09 - manipulacion de datos.html#lectura",
    "title": "Manipulación de datos",
    "section": "Lectura",
    "text": "Lectura\n\nPython for Data Analysis (cap. 7, 8 y 10)\nEncoding Categorical Predictors (Feature Engineering and Selection: A Practical Approach for Predictive Models: cap. 5)\nEngineering Numeric Predictors (Feature Engineering and Selection: A Practical Approach for Predictive Models: cap. 6)\nHandling Missing Data (Feature Engineering and Selection: A Practical Approach for Predictive Models: cap. 8)\nA Beginner’s Guide to Clean Data"
  },
  {
    "objectID": "09 - manipulacion de datos.html#actividades",
    "href": "09 - manipulacion de datos.html#actividades",
    "title": "Manipulación de datos",
    "section": "Actividades",
    "text": "Actividades"
  },
  {
    "objectID": "09 - manipulacion de datos.html#resumen",
    "href": "09 - manipulacion de datos.html#resumen",
    "title": "Manipulación de datos",
    "section": "Resumen",
    "text": "Resumen\nLa manipulación de datos (o data wrangling) implica la unión, transformación, agregación y/o filtrado de los datos, no necesariamente en ese orden. Es habitual encontrarse con el dicho de que el 80% del tiempo que dedica un analista de datos es a tareas relacionadas a esta cuestión, y posiblemente no sea una mala estimación.\nComenzando con la detección valores faltantes (que no siempre significarán lo mismo), atípicos o outliers (que no necesariamente siempre lo serán), registros duplicados (que no necesariamente siempre será incorrecto que estén duplicados), son sólo los primeros pasos en una serie de tareas complejas tanto en lo técnico como en lo conceptual, que efectivamente demandan mucho tiempo. A ello, se le sumará la necesidad de integrar diferentes fuentes de datos, realizando operaciones de unión y concatenación entre datasets (o tablas en una base de datos), y un anterior/posterior filtrado y/o agregación de los datos, no sólo para resumir la información o trabajar con unidades de análisis diferentes, sino también quizás para la creación de nuevas variables y/o transformación de las ya disponibles (lo cual se conoce como feature engineering y merece un capítulo completo aparte).\nLo importante a señalar es que el proceso en sí (todas las operaciones realizadas) debe ser conocido y comprendido por el/la analista de datos porque los resultados del análisis, principalmente cuando se trabaja con inferencia, dependerá fuertemente de lo realizado, por supuesto incluyendo el origen mismo de los datos.\nEn fin, en el mejor de los casos, dentro de las empresas/organizaciones con procesos de analítica bien consolidados, el rol del ingeniero/a de datos atacará estos aspectos y disponibilizará la información como se necesita. Sin embargo, más allá de que esto se cumpla, inevitablemente se requerirá de cierto grado de manipulación en los datos."
  },
  {
    "objectID": "10 - probabilidad aplicada.html#lectura",
    "href": "10 - probabilidad aplicada.html#lectura",
    "title": "Probabilidad aplicada",
    "section": "Lectura",
    "text": "Lectura\n\nEpílogo: Estadística y Probabilidad (Estadística para Todos: pág. 245 a 247)\nEstudio de los conceptos de la probabilidad (Estadística Aplicada a los Negocios y la Economía: pág. 144 a 151)\nDistribuciones de Probabilidad Discreta (Estadística Aplicada a los Negocios y la Economía: pág. 186 a 202)\nDistribuciones de Probabilidad Continua (Estadística Aplicada a los Negocios y la Economía: pág. 222 a 242)\nProbability (STAT 500 | Applied Statistics: lesson 2)\nProbability Distributions (STAT 500 | Applied Statistics: lesson 3)\nSampling Distributions (STAT 500 | Applied Statistics: lesson 4)\nIntroducción a conceptos de probabilidad y estadística descriptiva (Laboratorio de Datos – 1er Cuatrimestre 2021: Clase 4)"
  },
  {
    "objectID": "10 - probabilidad aplicada.html#actividades",
    "href": "10 - probabilidad aplicada.html#actividades",
    "title": "Probabilidad aplicada",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Pokemon. Proponga 3 experimentos sobre dicha población para obtener como resultado una variable aleatoria cuya distribución sea: (1) binomial, (2) normal y (3) chi cuadrado.\nResponda las siguientes preguntas:\n\n¿Cuál es la probabilidad de obtener un pokemon cuya velocidad sea mayor a 100?\n¿Cuál es la probabilidad de obtener un pokemon cuyo ataque sea igual a 60?\n¿Cuál es la probabilidad de obtener un pokemon de aire al realizar una muestra aleatoria de 10 pokemones?\n¿Cuál es la probabilidad de obtener un pokemon de generación 3 al seleccionar un sólo pokemon?\nDado que un pokemon es de tierra, ¿cuál es la probabilidad que su defensa sea menor a 50?\n¿Cuál es la probabilidad de obtener un pokemon de tierra y con defensa menor a 50?\n¿Cuál es la probabilidad de que un pokemon sea de tierra suponiendo que en una muestra de 10 unidades obtuvo sólo 1 pokemon de este tipo?"
  },
  {
    "objectID": "10 - probabilidad aplicada.html#resumen",
    "href": "10 - probabilidad aplicada.html#resumen",
    "title": "Probabilidad aplicada",
    "section": "Resumen",
    "text": "Resumen\nLa probabilidad es una herramienta fundamental para el análisis de datos, no tanto por su uso directo, sino porque es el sustento matemático de muchas demostraciones de procesos y fórmulas que habitualmente se aplican. Por tanto, conocer los conceptos y definiciones básicos es una puerta a comprender mejor tanto el análisis como las conclusiones obtenidas.\nSerá particularmente útil asimilar las distribuciones más relevantes que surgen como resultado del análisis de variables aleatorias; éstas son: la distribución binomial, la distribución normal y la distribución chi-cuadrado. Cada una estará asociada al estudio de medidas de posición, dispersión y relación, vistas anteriormente. Es decir, estas medidas, si se dan las adecuadas condiciones, tendrán un comportamiento teóricamente definido y demostrado, lo cual posibilita enormemente su análisis. También será útil el conocimiento y cálculo de probabilidades conjuntas y condicionales, habituales en el análisis multivariado."
  },
  {
    "objectID": "11 - estimacion puntual y por intervalos.html#lectura",
    "href": "11 - estimacion puntual y por intervalos.html#lectura",
    "title": "Estimación puntual y por intervalos de confianza",
    "section": "Lectura",
    "text": "Lectura\n\nEstimación y parámetros (Estadística para Todos: pág. 55)\nVariabilidad entre muestra y muestra (Estadística para Todos: pág. 58 a 63)\nTeorema Central del Límite (Estadística para Todos: pág. 200 a 207)\nDistribuciones de muestreo de estadísticas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 218 a 238)\nEstimación por intervalos (Estadística para Todos: pág. 214 a 228)\nEstimación e intervalos de confianza (Estadística Aplicada a los Negocios y la Economía: pág. 297 a 315)\nPropiedades deseables de los estimadores puntuales (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 251 a 261)\nConfidence Intervales (STAT 500 | Applied Statistics: lesson 5)"
  },
  {
    "objectID": "11 - estimacion puntual y por intervalos.html#actividades",
    "href": "11 - estimacion puntual y por intervalos.html#actividades",
    "title": "Estimación puntual y por intervalos de confianza",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Pokemon y asuma que es el listado completo de todos los pokemones (es decir, la población). Use los parámetros obtenidos previamente y vuelva a calcular estimadores (utilizando las fórmulas adecuadas para cada caso, vistas en esta sección) pero realizando muestras aleatorias de manera sucesiva. Realice este mismo procedimiento con muestras de diferente tamaño. Grafique la distribución de los estimadores y saque conclusiones.\nEstime nuevamente los valores de media, varianza y proporción para las variables seleccionadas, utilizando diferentes niveles de confianza. Escriba una breve reseña con conclusiones acerca de los resultados obtenidos."
  },
  {
    "objectID": "11 - estimacion puntual y por intervalos.html#resumen",
    "href": "11 - estimacion puntual y por intervalos.html#resumen",
    "title": "Estimación puntual y por intervalos de confianza",
    "section": "Resumen",
    "text": "Resumen\nEstimar es aproximar. Siendo que no se dispone, por razones operacionales o conceptuales, de toda la información disponible, habrá inevitablemente ciertas características de las variables que serán desconocidas; éstas pueden ser cualquiera de las medidas de posición, dispersión o relación vistas previamente. El verdadero valor de éstas es lo que recibirá el nombre de parámetro.\nEl objetivo será justamente aproximarse a estos parámetros a través de diferentes fórmulas, que reciben el nombre de estimadores. Para cada caso, no hay único estimador posible, aunque hay algunos que cumplirán con ciertas propiedades que los hacen óptimos o al menos mejores en comparación a otros.\nPara llegar a un valor aproximado de los parámetros, a través de los estimadores, serán necesario datos. Aquí entra en juego el proceso de muestreo, la obtención de datos, necesariamente aleatoria, para que la estimación arroje resultados válidos. Mientras mayor y más representativa sea una muestra respecto de la población, los valores de los estimadores estarán cada vez más cerca de los valores de los parámetros, siempre siendo distintos en cada muestra pero con una variación menor a medida que el tamaño de muestra tienda al número completo de unidades de análisis (o al infinito).\nLa variación en el resultado de los estimadores jugará un rol fundamental en la gestión de la incertidumbre. Es habitual no definir un valor tal para el estimador, sino más bien un intervalo. Se podrá decir que dicho intervalo, con cierto grado de confianza, contendrá al verdadero valor del parámetro en una cantidad de veces determinada (cantidad de muestras o de experimentos).\nTodos los resultados obtenidos serán sujetos al supuesto de que la población (de unidades de análisis) tiene determinada distribución. Al intentar obtener una estimación de la media, de la varianza o una proporción, se deberá tener en cuenta que justamente son parámetros utilizados en distribuciones particulares, algunas de las vistas anteriormente, como la normal o binomial. Eso, en cierta forma, facilitará los cálculos y dará mayor certidumbre respecto de las conclusiones; pero, en muchos casos, las distribuciones no son conocidas o bien no se intentará estimar algunos de estos parámetros. En ese caso, se estará frente a la necesidad de recurrir a métodos no paramétricos de estimación, los cuales se verán más adelante."
  },
  {
    "objectID": "12 - tests de hipotesis.html#lectura",
    "href": "12 - tests de hipotesis.html#lectura",
    "title": "Tests de hipótesis",
    "section": "Lectura",
    "text": "Lectura\n\nDecisiones en el campo de la estadística (Estadística para Todos: pág. 231 a 241)\nPruebas de hipótesis de una muestra (Estadística Aplicada a los Negocios y la Economía: pág. 333 a 346)\nPruebas de hipótesis estadísticas (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 303 a 346)\nHypothesis Testing for One-Sample Proportion (STAT 500 | Applied Statistics: lesson 6a)\nHypothesis Testing for One-Sample Mean (STAT 500 | Applied Statistics: lesson 6b)"
  },
  {
    "objectID": "12 - tests de hipotesis.html#actividades",
    "href": "12 - tests de hipotesis.html#actividades",
    "title": "Tests de hipótesis",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Breast Cancer Data. Asuma que es resultado de un muestreo aleatorio, llevado a cabo en el marco de una investigación, y responda/realice lo siguiente:\n\nDefina población y unidad de análisis.\nDetalle las posibles características del procedimiento de muestreo llevado adelante.\nRealice un breve análisis exploratorio.\nPlantee al menos 3 preguntas relevantes para la investigación y que puedan ser respondidas a partir de dichos datos.\nA partir de las preguntas, defina las pruebas de hipótesis correspondientes y calcule los estimadores necesarios.\n\n(ENTREGA OBLIGATORIA) Vuelque todo lo anterior en un informe (máx. 10 páginas). Incluya:\n\nUn resumen del trabajo realizado a modo de introducción, considerando supuestos, planteamientos y resultados.\nUna sección con el análisis de los datos y procedimientos correspondientes. Explique y justifique cada paso.\nUn apartado final donde exponga conclusiones acerca de su trabajo, formule interrogantes acerca de la validez de sus conclusiones y presente posibles caminos de acción para subsanar/mejorar/ampliar la investigación."
  },
  {
    "objectID": "12 - tests de hipotesis.html#resumen",
    "href": "12 - tests de hipotesis.html#resumen",
    "title": "Tests de hipótesis",
    "section": "Resumen",
    "text": "Resumen\nPartiendo de la base que se calculó determinado estimador (es decir, se obtiene una aproximación de alguna característica de una variable), muchas veces lo que se busca es contrastar el resultado con alguna suposición acerca del valor real o quizás contra un valor dado por cierto. Esa puesta a prueba de lo conocido, recibirá el nombre de test de hipótesis. El test de hipótesis, haciendo uso de la evidencia (datos) relevada, buscará rechazar (o no) lo que se considera verdadero hasta el momento. Cabe señalar que justamente así funciona la ciencia en términos conceptuales, por tanto, los test de hipótesis son la herramienta por excelencia para la investigación científica.\nAsí como el cálculo de los estimadores, el resultado de un test de hipótesis dependerá de la calidad de los datos obtenidos, de la representatividad de la muestra. Esta obtención de datos debe ser aleatoria, al menos en algún punto del procedimiento general, y estará sujeta además a la suposición o no de que la variable (sobre la cual se testea) tenga determinada distribución. De no ser así, y al igual que con la estimación, se recurrirá a métodos no paramétricos.\nPor otro lado, las pruebas estadísticas (otra denominación para los tests) variarán según se cuente con una o más muestras, una o más variables, variables independientes o apareadas (por ejemplo, la misma variable en dos momentos de tiempo diferentes), dispersión conocida o desconocida, entre otras cuestiones. Es una batería realmente extensa que dificilmente se pueda memorizar, con lo cual es recomendable más bien asimilar el reconomiento de escenarios frente a los cuales se puede aplicar un test y luego indagar en cuál sería el más apropiado para el mismo. En ese sentido, es una aproximación similar a la de la visualización de datos, pero para la cual se deberá ser mucho más precabido ya que, siendo una herramienta crítica para la obtención de conclusiones, puede conducir a acciones incorrectamente fundamentadas."
  },
  {
    "objectID": "13 - estadistica no parametrica.html#lectura",
    "href": "13 - estadistica no parametrica.html#lectura",
    "title": "Estadística NO paramétrica",
    "section": "Lectura",
    "text": "Lectura\n\nPrueba sobre la mediana\n\nPrueba de los signos (Estadística Aplicada a los Negocios y la Economía: pág. 681 a 685)\nInference for the Population Median (STAT 500 | Applied Statistics: lesson 11.1)\nDistribution-Free Confidence Intervals for Percentiles (STAT 415 | Introduction to Mathematical Statistics: lesson 19)\n\nPruebas sobre distribuciones (bondad de ajuste / independencia / aleatoriedad)\n\nMétodos no paramétricos: pruebas de bondad de ajuste (Estadística Aplicada a los Negocios y la Economía: pág. 648 a 658)\nPrueba de hipótesis de que la distribución de datos proviene de una población normal (Estadística Aplicada a los Negocios y la Economía: pág. 659 a 664)\nAnálisis de tablas de contingencia (Estadística Aplicada a los Negocios y la Economía: pág. 667 a 670)\nLa prueba de bondad de ajuste chi-cuadrada (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 363 a 368)\nLa estadística de Kolmogorov-Smirnov (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 368 a 370)\nChi-Square Test for Independence (STAT 500 | Applied Statistics: lesson 8)\nChi-Square Goodness-of-Fit Tests (STAT 415 | Introduction to Mathematical Statistics: lesson 16)\nContingency Tables (STAT 415 | Introduction to Mathematical Statistics: lesson 17)\nThe Wilcoxon Tests (STAT 415 | Introduction to Mathematical Statistics: lesson 20)\nRun Test and Test for Randomness (STAT 415 | Introduction to Mathematical Statistics: lesson 21)\nKolmogorov-Smirnov Goodness-of-Fit Test (STAT 415 | Introduction to Mathematical Statistics: lesson 22)\n\nBootstrapping\n\nIntroduction to Bootstrapping (STAT 500 | Applied Statistics: lesson 11.2)\nBootstrapping Main Ideas!!! (StatQuest YouTube Channel)"
  },
  {
    "objectID": "13 - estadistica no parametrica.html#actividades",
    "href": "13 - estadistica no parametrica.html#actividades",
    "title": "Estadística NO paramétrica",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Wendy’s Menu Nutrition Data. Asuma que es resultado de un muestreo aleatorio de productos y responda/realice lo siguiente:\n\nRealice la prueba de Kolmogorov-Smirnov sobre todas las variables y concluya cuáles de ellas no tienen distribución normal. Complemente con métodos gráficos si considera necesario.\nUtilizando el método de boostrapping, lleve adelante la estimación de media, mediana y desviación estándar para todas aquellas variables.\nSe ha afirmado que el 50% de los productos tiene más de 800 calorías. ¿Está usted de acuerdo? Evalúe y compare distintos métodos para llegar a una conclusión.\nSe ha afirmado que la valoración de los productos está relacionada a su cantidad de grasa. ¿Está usted de acuerdo? Tenga en cuenta adecuar los datos para poder realizar el correspondiente análisis."
  },
  {
    "objectID": "13 - estadistica no parametrica.html#resumen",
    "href": "13 - estadistica no parametrica.html#resumen",
    "title": "Estadística NO paramétrica",
    "section": "Resumen",
    "text": "Resumen\nPrimero y principal, hay muchas hipótesis que escapan del simple cuestionamiento de si un valor (estimado) es igual a otro, como se vió en el capítulo anterior. Por ejemplo, se puede querer comprobar (rechazar o no) si una variable tiene o no cierta distribución, sin necesidad del conocimiento o estimación de sus parámetros. Bajo un concepto similar, quizás se pretenda corroborar (rechazar o no) si una variable es independiente de otra o si es incluso aleatoria, lo cuales son supuestos en gran parte de los análisis. Siendo así, que no se busca la obtención ni se recurre al uso o definición de algún parámetro de interés (al menos no uno asociado a una distribución conocida, como la mediana), surge una batería de métodos / tests / pruebas denominados no parámetricos.\nAsí como con los tests parámetricos, la cantidad disponible de situaciones en los cuales pueden aplicarse son incontables, y es preferible disponer de la habilidad de reconocer cuándo (o no) es apropiado su uso y bajo qué condiciones. Cabe señalar que este tipo de pruebas, muy propias del ámbito estadístico, suelen hallarse implementadas de manera más completa o detallada en el lenguaje R, y es recomendable su uso siendo que hay mayor cantidad de documentación e información académica disponible.\nEn los últimos años, de la mano de la ciencia de datos y del incremento en la potencia de cálculo computacional, se ha popularizado bastante el uso de un método no paramétrico en particular: boostrap. En esencia, implica el remuestreo (es decir, la muestra de una muestra) con reposición, de tal manera de generar un conjunto de muestras mayor al que se dispone inicialmente (posiblemente una sola). Para cada muestra, se obtiene un estimador (de la manera habitual que ya se ha visto) y la distribución de valores estimados resultantes justamente pasa a ser el valor agregado de este método, que de otra manera se obtendría mediante muestras reales (operacionalmente imposible en la mayoría de los casos). Es importante señalar que, bajo este procedimiento, sumamente simple, se puede estimar cualquier parámetro o característica de interés de una variable. Solamente es necesario una muestra, la cual pasa a ser crucial en términos de calidad y representatividad, y un soporte para el remuestreo (una simulación por software). El bootstrapping se encuentra implementado en python / R, muy bien documentado y con muchos ejemplos, dada su uso frecuente hoy día."
  },
  {
    "objectID": "14 - tecnicas de muestreo.html#lectura",
    "href": "14 - tecnicas de muestreo.html#lectura",
    "title": "Técnicas de muestreo",
    "section": "Lectura",
    "text": "Lectura\n\nMétodos de muestreo (Estadística Aplicada a los Negocios y la Economía: pág. 266 a 271)\nElección del tamaño adecuado de una muestra (Estadística Aplicada a los Negocios y la Economía: pág. 316 a 321)\nEstimating Population Mean and Total under SRS (STAT 506 | Sampling Theory and Methods: lesson 1)\nConfidence Intervals and Sample Size (STAT 506 | Sampling Theory and Methods: lesson 2)\nStratified Sampling (STAT 506 | Sampling Theory and Methods: lesson 6)\nCluster and Systematic Sampling (STAT 506 | Sampling Theory and Methods: lessons 7 y 8)\nMulti-Stage Designs (STAT 506 | Sampling Theory and Methods: lesson 9)"
  },
  {
    "objectID": "14 - tecnicas de muestreo.html#actividades",
    "href": "14 - tecnicas de muestreo.html#actividades",
    "title": "Técnicas de muestreo",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset LinkedIn Freelancer Survey Results. Señale algunos aspectos claves a considerar en la realización de dicha encuesta (¿acaso podría implementarse un muestreo estratificado, por conglomerados o sistemático?) y haga una estimación del tamaño adecuado para la misma.\nConsidere el dataset London Bike Sharing Dataset. Asuma que es el total de registros (por hora) de viajes en bicicleta hasta el día de la fecha. Realice un análisis exploratorio de la variable cnt (total de viajes), principalmente calculando la media y desvío estándar. Luego, implemente un muestreo sistemático de los datos, estime los paramétros correspondientes y compare. Realice dicha comparación en base a temporada (season) y fines de semana (weekend)."
  },
  {
    "objectID": "14 - tecnicas de muestreo.html#resumen",
    "href": "14 - tecnicas de muestreo.html#resumen",
    "title": "Técnicas de muestreo",
    "section": "Resumen",
    "text": "Resumen\nLas situaciones reales para la obtención de datos dificilmente permiten que un muestreo aleatorio simple sea llevado adelante. Por ejemplo, cuestiones como los costos operacionales debido a la extensión de territorio son comunes en cualquier estudio de estadísticas sociales. Es así que han surgido diferentes técnicas de muestreo y estimadores asociadas a las mismas, de tal manera que la aproximación a los parámetros reales de la población sea la mejor (que es el objetivo de cualquier estimador).\nDicho eso, hay una consideración inicial muy importante al comenzar el muestreo: el tamaño de la muestra. Si es muy pequeño, la representatividad podría no ser adecuada (con todo lo que ello implica y que se ha señalado anteriormente); si es muy grande, es esencialmente gasto innecesario de recursos. Por tanto, determinar un adecuado tamaño de muestra es el puntapié para un buen análisis de datos, que estará atado luego a la técnica con la cual se realiza el muestreo.\nEn ese sentido, hay 2 que recursos que destacan: el uso de estratos y el uso de conglomerados.\nLos primeros (estratos) están pensados para estudiar regiones o agrupamientos de interés para la investigación, obteniendo estadísticas no sólo a nivel general sino al interior de cada uno. Es una división que puede realizarse sobre la población (el conjunto total de unidades de análisis) antes o después del muestreo (post estratificación), con la condición de estudiar el conjunto completo de estratos, ya que posiblemente cada estrato sea una representación parcial de la población. Sobre cada estrato se realiza una muestra aleatoria que luego se estudia en conjunto con el resto. Un ejemplo de estratos pueden ser los barrios de una ciudad.\nLos segundos (conglomerados) son agrupamientos de unidades de análisis que suelen presentarse naturalmente juntos y que cuentan con la particularidad de ser representaciones de la población por sí mismos, así como también de facilitar el estudio / muestreo. Los conglomerados no se muestrean, sino que se relevan todas las unidades al interior de los mismos. La unidad de muestreo es entonces el conglomerado, siendo éstos los que se seleccionan, habitualmente de forma sistemática. Un ejemplo de conglomerados podrían ser los pasajeros de un vuelo."
  },
  {
    "objectID": "15 - anova.html#lectura",
    "href": "15 - anova.html#lectura",
    "title": "ANOVA: Análisis de la varianza",
    "section": "Lectura",
    "text": "Lectura\n\nAnálisis de la varianza (Estadística Aplicada a los Negocios y la Economía: pág. 410 a 440)\nIntroduction to ANOVA (STAT 500 | Applied Statistics: lesson 10)"
  },
  {
    "objectID": "15 - anova.html#actividades",
    "href": "15 - anova.html#actividades",
    "title": "ANOVA: Análisis de la varianza",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset CNC turning Experiment. Identifique aquellos factores para los cuales la media de la fuerza (force), en al menos un grupo, sea significativamente diferente. Evalué en todos los casos los supuestos del análisis. Si para un grupo de factores encuentra diferencias, determine entre cuáles. Concluya cuáles factores son los que determinan la mayor fuerza posible alcanzada.\nLleve adelante una transformación de los datos tal que las réplicas de cada corrida (run_id) pasen a conformar las columnas r1 a r6 y además calcule la media y desvío estándar asociadas a las mismas."
  },
  {
    "objectID": "15 - anova.html#resumen",
    "href": "15 - anova.html#resumen",
    "title": "ANOVA: Análisis de la varianza",
    "section": "Resumen",
    "text": "Resumen\nEn la práctica, la comparación de promedios (a través de pruebas estadísticas) suele implicar más de dos categorías, grupos y/o variables. Consideren, por ejemplo, un experimento con 6 diferentes tratamientos, el resultado de cada uno de ellos variará, en mayor o menor medida, y como investigador se intentará conocer esa variación, concluyendo si uno o más son realmente diferentes al resto en cuanto a sus efectos. Justamente, el análisis de dicha variación es lo que se conoce como ANOVA (análisis de la varianza).\nEsta técnica es muy simple en esencia. Se compara la variación dentro de cada grupo, respecto a la variación entre grupos. Si la última es considerablemente grande (y la primera considerablemente chica) se estará en condiciones de decir que los grupos son diferentes. Dicho eso, según se quiera poner a prueba diferentes variables (factores) que afectan el resultado, el ANOVA como tal puede llegar a ser bastante complejo. Sea cual fuere el escenario, es muy importante tener presente qué es lo que implicará la introducción como tal de más componentes al análisis, consideración clave para obtener conclusiones válidas. Un caso importante en ese aspecto es el uso de factores de bloqueo, los cuales serán instrumentaciones del experimento realizadas a propósito para controlar la variación de la respuesta.\nPosteriormente a las conclusiones del ANOVA, suelen darse otros procedimientos para específicamente conocer cuál tratamiento / grupo es diferente al resto. Esto es lo que se conoce como análisis post hoc.\nANOVA, siendo una prueba estadística, es un procedimiento muy utilizado en investigación científica; pero es destacable que no debe vérselo simplemente como un test sino como una forma de estimar una variable continua en función de una o más variables categóricas, lo que se denomina, en términos generales, como una regresión (tema que se verá en el próximo capítulo). También es importante señalar que el análisis de la varianza, tal como se presenta aquí, es un método paramétrico, teniendo su contrapartida no paramétrica para el caso donde no se cumplan los supuestos (o bien se investigue otro parámetro que no sea el promedio)."
  },
  {
    "objectID": "16 - regresion lineal.html#lectura",
    "href": "16 - regresion lineal.html#lectura",
    "title": "Regresión Lineal",
    "section": "Lectura",
    "text": "Lectura\n\nConceptos generales\n\nRelación entre variables (Estadística para Todos: pág. 168 a 196)\nEl estudio de la relación entre variables (Estadística Aplicada a las Ciencias Sociales y Humanas: pág. 105 a 130)\n\nRegresión lineal\n\nRegresión lineal y correlación (Estadística Aplicada a los Negocios y la Economía: pág. 461 a 494)\nAnálisis de regresión múltiple (Estadística Aplicada a los Negocios y la Economía: pág. 513 a 542)\nAnálisis de regresión: el modelo lineal simple (Probabilidad y Estadística | Aplicaciones y Métodos: pág. 443 a 478)\nFundamentals of Data Visualization (Part I: Cap. 12)\nLinear Regression Foundations (STAT 500 | Applied Statistics: lesson 9)\nLinear Regression (Interpretable Machine Learning: Cap. 5.1)\nRegresión Lineal (Laboratorio de Datos – 1er Cuatrimestre 2021: Clase 6)\n\nImplementación en Python\n\nIntroduction to scikit-learn (Python for Data Analysis: cap. 12)\nLinear Regression in Python (Real Python site)\nIn Depth: Linear Regression (Python Data Science Handbook)"
  },
  {
    "objectID": "16 - regresion lineal.html#actividades",
    "href": "16 - regresion lineal.html#actividades",
    "title": "Regresión Lineal",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Real State Pricing y asuma que es resultado de un muestreo aleatorio en determinada zona urbana. Construya un modelo de regresión lineal múltiple para estimar el precio (por unidad de superficie) de las casas. Realice el correspondiente análisis exploratorio de los datos, identifique posibles problemáticas y evalúe los resultados, tanto de las estimaciones como de los residuos de las mismas. Utilice la librería sklearn de Python para el desarrollo de este análisis."
  },
  {
    "objectID": "16 - regresion lineal.html#resumen",
    "href": "16 - regresion lineal.html#resumen",
    "title": "Regresión Lineal",
    "section": "Resumen",
    "text": "Resumen\nUno de los modelos más ampliamente difundidos, utilizado para la estimación de una variable continua a partir de variables continuas y/o variables cualitativas (binarias). Implica, primeramente, la estimación de ciertos parámetros asociados a cada variable independiente, que constituyen una expresión matemática a partir de la cual, estimados esos valores, se llega a una aproximación de la variable independiente.\nSobre esta aproximación, será relevante evaluar cuán cerca se encuentra de los valores efectivamente observados y si los residuos resultantes de esa evaluación presentan un comportamiento particular, lo cual será una señal de cuán bueno es dicho modelo, buscando ser una representación lo más completa posible del verdadero modelo que subyace a los datos.\nCabe destacar que, como todo modelo, es una simplificación matemática de la realidad, con lo cual no necesariamente se llegue a los resultados esperados. Las estimaciones pueden estar lejos de ser las observadas, posiblemente porque se está dejando de lado otras variables que tienen impacto o bien porque la naturaleza del fenónemo en sí no es lineal en sus parámetros (una incorrecta especificación del modelo), entre varias otras razones. Retomando lo dicho al principio, es un modelo ampliamente utilizado, pero no por eso de manera correcta. Los datos pueden presentar un conjunto de problemáticas tales como multicolinealidad, heterocedasticidad, autocorrelación, que deben abordarse debidamente y con ello ahorrar tiempo en concluir que el modelo de regresión lineal en verdad no será adecuado.\nSe pueden encontrar implementaciones muy completas tanto en python y R, en múltiples librerías. En el ámbito de ciencia de datos / machine learning, es habitual el uso de sklearn, considerado incluso un estándar en cuanto a su formato de construcción de modelos. Esta librería será justamente la que se utilizará de aquí en adelante para el resto de los capítulos."
  },
  {
    "objectID": "17 - regresion logistica.html#lectura",
    "href": "17 - regresion logistica.html#lectura",
    "title": "Regresión Logística",
    "section": "Lectura",
    "text": "Lectura\n\nLogistic Regression (Introduction to Statistical Learning: pág. 133 a 139)\nLogistic Regression (Interpretable Machine Learning: Cap. 5.2)\nLogistic Regression (Introduction to Modern Statistics: Cap. 9)\nLogistic Regression in Python (Real Python site)\nRegresión Logística (Laboratorio de Datos – 1er Cuatrimestre 2021: Clase 8)"
  },
  {
    "objectID": "17 - regresion logistica.html#actividades",
    "href": "17 - regresion logistica.html#actividades",
    "title": "Regresión Logística",
    "section": "Actividades",
    "text": "Actividades\n\nConsidere el dataset Real State Pricing y asuma que es resultado de un muestreo aleatorio en determinada zona urbana. Transforme los datos de tal manera que pueda construir un modelo de regresión logística múltiple y luego estimar si el precio (por unidad de superficie) de las casas estará por encima o por debajo de los 50 USD. Evalúe los resultados utilizando una matriz de confusión y concluya cuáles podrían ser las consecuencias de aplicar el modelo en un caso real de estimación de precios. Utilice la librería sklearn de Python para el desarrollo de este análisis."
  },
  {
    "objectID": "17 - regresion logistica.html#resumen",
    "href": "17 - regresion logistica.html#resumen",
    "title": "Regresión Logística",
    "section": "Resumen",
    "text": "Resumen\nLa regresión logística es, en esencia, un modelo de clasificación binaria (al menos en su forma básica), utilizado ampliamente en la industria crediticia desde hace muchos años y que cuenta con la particularidad de ser, al igual que la regresión lineal, interpretable en cuanto a la toma de decisiones. Es un modelo intrínsecamente lineal y las estimaciones de los parámetros obtenidos son accesibles, pudiendo establecerse conclusiones individuables sobre la importancia que conlleva cada variable sobre la estimación final de la variable dependiente así como también aplicar pruebas estadísticas sobre dichos parámetros.\nA diferencia de la regresión lineal, el resultado del modelo no será una cantidad en unidades medibles, sino más bien una probabilidad, estando ésta entre 0 y 1. En ese sentido, se pueden establecer diferentes criterios para finalmente optar por un valor u otro, utilizando habitualmente el punto medio (0.5) como punto de corte. Así también, suele presentarse el escenario en el cual se busca clasificar correctamente en pos de una u otra categoría (por ejemplo, en medicina donde es preferible decirle a un paciente que está enfermo a que no lo está) y para ello se buscará que el modelo se ajuste a obtener resultados propensos en una u otra dirección. Las medidas más utilizadas para ello son la precisión, el recall, la tasa de falsos y verdaderos positivos, y varias otras medidas disponibles a través de la construcción de la matriz de confusión, herramienta por excelencia para la evaluación, no sólo de la regresión logística, sino de los modelos de clasificación (o clasificadores) en general.\nCon el auge de la ciencia de datos / machine learning, la regresión logística ha ido dando lugar a otros modelos más sencillos de calcular / interpretar como los árboles de decisión ó random forests y que además no necesitan de supuestos sobre la distribución de los datos (el modelo en sí) para su uso y posterior elaboración de conclusiones. En el siguiente capítulo, se verán algunos de ellos."
  },
  {
    "objectID": "18 - knn y cart.html#lectura",
    "href": "18 - knn y cart.html#lectura",
    "title": "KNN y CART",
    "section": "Lectura",
    "text": "Lectura\n\nConceptos generales\n\nModel-Agnostic Methods (Interpretable Machine Learning: Cap. 6)\nExample-Based Explanations (Interpretable Machine Learning: Cap. 7)\n\nKNN\n\nK-Nearest Neighbors (Introduction to Statistical Learning: pág. 39 a 42)\nNearest Neighbors (sklearn User Guide)\nThe k-Nearest Neighbors (kNN) Algorithm in Python (Real Python site)\nComparison of Linear Regression with K-Nearest Neighbors (Introduction to Statistical Learning: pág. 105 a 110)\n\nCART\n\nDecision Tree (Interpretable Machine Learning: Cap. 5.4)\nTree-Based Methods (Introduction to Statistical Learning: pág. 327 a 339)\nDecision Trees (sklearn User Guide)\nÁrboles de Decisión y Random Forest"
  },
  {
    "objectID": "18 - knn y cart.html#actividades",
    "href": "18 - knn y cart.html#actividades",
    "title": "KNN y CART",
    "section": "Actividades",
    "text": "Actividades\n\n(ENTREGA OBLIGATORIA) Considere el dataset Real State Pricing y asuma que es resultado de un muestreo aleatorio en determinada zona urbana. Estime el precio (por unidad de superficie) de las casas usando KNN y CART. Realice las transformaciones de datos que considere necesarias. Compare los resultados con los modelos previamente realizados (regresión lineal y logística). Utilice la librería sklearn de Python para el desarrollo de este análisis. El entregable de este trabajo debe ser una Jupyter notebook con los siguientes apartados: EDA, transformación de los datos, construcción de los modelos, comparación de resultados y conclusiones."
  },
  {
    "objectID": "18 - knn y cart.html#resumen",
    "href": "18 - knn y cart.html#resumen",
    "title": "KNN y CART",
    "section": "Resumen",
    "text": "Resumen\nEstos modelos, aplicables tanto para regresión como clasificación, son excelentes aproximaciones a los datos, sin contar necesariamente con conocimiento previo de su comportamiento ni realizar suposiciones acerca del modelo subyacente. Son útiles tanto en la etapa de exploración de datos o para construir modelos de base (porque en la práctica no son los suficimiente performantes como para implementar de manera productiva, por tanto sirven como punto de comparación para construir modelos que los superen).\nAl igual que la regresión lineal y logística, sus resultados son interpretables, aunque no cuentan con parámetros estimados que puedan analizarse posteriormente. Para el caso de los árboles de decisión, cuentan además con la ventaja de poder ingestar cualquier tipo de datos, ya que, en esencia, son reglas de “si o no” en función del poder discriminativo de los datos disponibles. En ese sentido, siendo que dependen exclusivamente de los datos, son muy sensibles a la calidad de los mismos y además al azar respecto al punto de partida seteado por el analista. Por tanto, debe tenerse en cuenta no establecer conclusiones apresuradas solamente a partir de estos modelos.\nMás adelante se verá que éstos son bloques a partir de los cuales se pueden construir modelos más complejos (muy utilizados en la industria, como random forests), con las mismas características, pero salvando las desventajas mencionadas. ."
  },
  {
    "objectID": "a01 - fundamentos de programacion.html#lectura",
    "href": "a01 - fundamentos de programacion.html#lectura",
    "title": "Fundamentos de programación",
    "section": "Lectura",
    "text": "Lectura\n\nPython for Everybody (Cap. 3 a 11 y 15)\nPython Tutorial (w3schools)\nPython Built-in Methods (Efficient Python Tricks and Tools for Data Scientists: Cap. 2)"
  },
  {
    "objectID": "a01 - fundamentos de programacion.html#actividades",
    "href": "a01 - fundamentos de programacion.html#actividades",
    "title": "Fundamentos de programación",
    "section": "Actividades",
    "text": "Actividades\n\nRealice en Python (o en el lenguaje que usted prefiera) el siguiente listado de ejercicios, poniendo en práctica el uso de estructuras básicas de datos, operaciones y funciones:\n\nDada una lista de números y un número cualquiera, escriba un función que permita identificar dentro de la lista 2 elementos cuya suma sea dicho número.\nDado un número, escriba una función que permita identificar si el mismo es palíndromo (un número que se lee igual al derecho o al revés).\nEscriba una función que permita convertir un número romano a números decimales (los números que usamos habitualmente).\nDada una oración (compuesta de palabras y espacios), escriba una función que permita obtener el largo (cantidad de caracteres) de la última palabra.\nEscriba una función para obtener la moda de una lista de números. Debe devolver como resultado una tupla con el número y su frecuencia.\nEscriba una función que devuelva True o False si un texto (un string) presenta palabras repetidas.\nImplemente una clase llamada Notebook con un método denominado write() que permita almacenar notas (un string cualquiera) con su correspondiente título y otro método list() que devuelva el listado completo de notas almacenadas. Considere la estructura de datos más adecuada según su criterio."
  },
  {
    "objectID": "a01 - fundamentos de programacion.html#resumen",
    "href": "a01 - fundamentos de programacion.html#resumen",
    "title": "Fundamentos de programación",
    "section": "Resumen",
    "text": "Resumen\nHay conceptos que son comunes a toda la programación. Por tanto, independientemente de que uno utilice Python, R, Julia, Javascript, SQL, etc. siempre encontrará la forma de desarrollar aquello que necesita recurriendo al reconocimiento y uso de diferentes tipos y estructuras de datos/variables, operadores y estructuras de control/flujo y funciones/procedimientos. Desde ya, en cada lenguaje habrá variaciones que tendrán que ser asimiladas oportunamente, pero la esencia estará en la funcionalidad de cada uno de los elementos recién mencionados.\nEs así que, primeramente, se tienen los datos y/o variables. Cabe repasar la sección de Datos, unidades y variables de análisis, ya que de nuevo se presenta al dato como la mínima unidad con la que se trabaja. Los datos, que pueden ser de uno u otro tipo, son almacenados en variables y esas variables pueden tener estructuras diversas, según sea la lógica de almacenamiento de la información. Por ejemplo, una estructura muy básica es una tabla o matriz, un arreglo de elementos en 2 dimensiones (horizontal y vertical), las cuales se ha señalado anteriormente que pueden utilizarse como medio para presentar agregaciones de datos (ver Tablas de frecuencias). Desde ya, los datos pueden traer consigo relaciones cuan complejas son en el mundo real, con lo cual existen estructuras de datos (o combinaciones de éstas) tales que puedan representar dicha complejidad. En python, se tienen listas, tuplas, arrays, diccionarios, dataframes y muchas más, habitualmente utilizados para el análisis de datos. En R y demás lenguajes pueden o no recibir el mismo nombre, así como también pueden variar las operaciones que se permiten realizar sobre las mismas.\nLas operaciones son la manera de relacionar elementos. En la práctica, se puede comenzar con el paralelismo con las operaciones matemáticas (suma, resta, multiplicación, etc.), lo cual es correcto, dado que están presentes en casi cualquier proceso de análisis. Un nivel por debajo de eso, existen las operaciones booleanas, que son la base de toda la programación. Se tratan de operaciones con unos y ceros, simples en naturaleza, pero igualmente muy presentes en el trabajo habitual con datos, ya que permiten la comparación y condicionamiento de elementos y procesos, a través de estructuras previamente pensadas para ello.\nToda operación con datos, variables y/o estructuras de datos posiblemente deba ser reutilizada (en el lenguaje que sea), por lo tanto entra en juego lo que se denominan (casi universalmente) como funciones. Las funciones son bloques de operaciones que, habitualmente, tienen datos como entrada y salida, con alguna transformación de por medio. Cabe señalar que puede no ser así, pero que al menos un cambio (en el ámbito del código o entorno de trabajo) realizan. El conjunto de funciones suele recibir el nombre de librería (o módulo), que serán de vital importante, dado que es prácticamente imposible construir algo desde cero. De ahí la importancia de la reutilización (y de los conceptos de transparencia, replicabilidad, interoperabilidad y escalabilidad mencionados en Entorno de trabajo).\nA todo lo anterior corresponde agregarle una cuestión relacionada a los patrones de diseño en programación, esto es, a la forma en la que se desarrollan los programas, dado que pueden llegar a ser extremadamente complejos y debe haber un criterio común en cuanto a la creación, no sólo de funciones, sino de cualquier elemento reutilizable. En ese sentido, se ha adoptado popularmente la programación orientada a objetos, que tiene como bloque de construcción a las clases, las cuales pueden ser una abstracción de cualquier entidad que pueda pensarse (autos, casas, sentimientos). A partir de esas clases, se pueden generar elementos concretos, lo que se denomina una instancia de dicha clase (un auto, una casa, un sentimiento en particular). Las instancias tendrán asociados un conjunto de atributos (el color de un auto, la altura de una casa, el fervor de un sentimiento) y operaciones, denominadas métodos (conducir el auto, pintar la casa, hacer reir), con los cuales podrá relacionarse al resto de los elementos y al entorno de programación mismo."
  }
]