# Entorno de trabajo

## Lectura

- Planillas de cálculo
    - [Tareas básicas en Excel (_Microsoft Support_)](https://support.microsoft.com/es-es/office/tareas-b%C3%A1sicas-en-excel-dc775dd1-fa52-430f-9c3c-d998d1735fca)
    - [Google Sheets (_Google Cloud Skills Boost_)](https://www.cloudskillsboost.google/course_templates/196)

- Jupyter Notebooks y otras plataformas similares
    - [Python Language Basics, IPython, and Jupyter Notebooks (_Python for Data Analysis_: cap. 2.1 y 2.2)](https://wesmckinney.com/book/python-basics.html)
    - <https://docs.jupyter.org/en/latest/use/using.html>
    - <https://colab.research.google.com/>
    - <https://posit.co/>
    - <https://code.visualstudio.com/>
    - <https://deepnote.com/>
    - <https://www.datacamp.com/workspace>
    - <https://observablehq.com/>

- Herramientas de visualización y consulta de datos
    - <https://powerbi.microsoft.com/en-au/>
    - <https://www.tableau.com/>
    - <https://www.metabase.com/>

- Suites de analítica de datos
    - <https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform>
    - <https://aws.amazon.com/es/sagemaker/>
    - <https://www.paperspace.com/gradient>
    - <https://learn.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning>

- Versionado
    - <https://git-scm.com/>
    - <https://docs.github.com/en/get-started/quickstart/hello-world>
    - <https://dvc.org/>

- Gestores de ambientes
    - <https://www.anaconda.com/>

- Conteinerización
    - <https://www.docker.com/>
    - <https://www.digitalocean.com/community/tutorials/el-ecosistema-de-docker-una-vision-general-de-la-contenerizacion-es>
    - <https://aws.amazon.com/es/docker/>

## Actividades

- Instale Docker Desktop (<https://www.docker.com/products/docker-desktop/>) en su computadora. Luego clone el siguiente repositorio <https://github.com/ervinwilneder-untref/statistics-playground> y levante el servicio de JupyterLab.

## Resumen

Uno de los puntos de fricción más habituales para quien comienza con análisis de datos es la **abrumadora cantidad de herramientas y entornos de trabajo** disponibles. Incluso habiéndose decidido, suele no ser tarea sencilla su configuración y la curva de aprendizaje es lenta, particularmente para el caso de aquellas personas sin un background en programación. Sin embargo, junto con el claro entendimiento de la teoría y práctica, **el buen uso de las herramientas es un factor clave** en el éxito profesional. 

Por lo tanto, antes de comenzar a realizar tareas propias de un/a analista de datos, se hace necesario transitar el panorama de plataformas destacadas, en cada una de las distintas cuestiones relacionadas a un correcto desarrollo/trabajo.

Comenzando con las **planillas de cálculo**, éstas son las herramientas con más trayectoria dentro del ámbito. Más allá de que hoy hay un sinfín de tareas complejas y volumen de datos que no pueden manejar, suele ser en el tramo final de un proyecto (la disponibilización de resultados para usuarios finales) donde están presentes, tanto por su sencillez así como también porque los mismos usuarios están acostumbrados a su uso.

Luego están las _notebooks_, generalmente asociadas Jupyter, que son esencialmente **documentos con la capacidad de volcar texto y código** (python, r, etc.), con una **dinámica de ejecución por celdas**, haciendo de las mismas la herramientas por excelencia para lo/as científicos de datos. Su uso actualmente se ha ido extendiendo y prácticamente **cualquier rol involucrado a datos comprende su uso y características**, considerándose algo así como un estándar. Mediante la implementación de diferentes _kernels_, las notebooks integran varios lenguajes, aunque su uso suele estar más asociado a Python. Para aquellos que programan en R, el uso de RStudio (ahora llamado Posit) resulta más cómodo, aunque ahora se está volviendo una plataforma con soporte para más lenguajes y puede llegar a ser una excelente opción como entorno de trabajo.

También hay otras herramientas que copian la misma dinámica de las notebooks y/o la extienden con nuevas funcionalidades. En ese sentido, el abanico de opciones es bastante amplio, destacándose principalemente _Colab_, por ser gratuita y con una capacidad de procesamiento generosa; o _JupyterLab_, que es una _IDE_ que permite la gestión de notebooks, instalación de plugins y más funcionalidades. Ésta última de hecho es adoptada por varias empresas para montar sobre la misma **servicios mucho más potentes y on-cloud** como GCP, AWS y Azure, conformando _suites_ de analítica de datos, en constante adopción por las empresas y organizaciones de hoy.

A lo anterior, debe sumarme las herramientas exclusivamente pensadas para **visualización de datos**, que comienzan actualmente a integrar capacidades de consulta (antes sólo atribuibles a un **cliente de bases de datos**) y transformación de datos (más allá de las **herramientas de ETL** propiamente dichas) dentro del mismo entorno.

Como se habrá podido concluir, este primer vistazo ya trajo consigo bastantes nuevas cosas para investigar y comprender. Lo importante, a fin de cuentas, es tener suficiente **flexibilidad** para adoptar sobre la marcha las herramientas que se dispongan en el trabajo o proyectos particulares. No se tiene que dejar pasar que habrá ciertas funcionalidades comunes a todas aquellas así como también ciertas propiedades deseables (no sólo para las herramientas, sino para cualquier análisis de datos en sí): **transparencia, replicabilidad, interoperabilidad y escalabilidad**.

Teniendo en cuenta ello, entran en juego otra batería de herramientas importantes. Se destacan principalmente: _Git_ para la transparencia (versionado y experimentación) y _Docker_ / _Anaconda_ para la replicabilidad (definición y gestión de dependencias). Considerar que no vale de nada un desarrollo que no pueda ser replicado o utilizado correctamente por otros colaboradores o incluso usuarios finales, en la era del **trabajo colaborativo** estas herramientas son prácticamente mandatorias.


